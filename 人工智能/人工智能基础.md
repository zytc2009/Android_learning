[Toc]

## 数学基础

### 线性代数

线性代数是用虚拟数字世界表示真实物理世界的工具。

在线性空间中，任意一个向量代表的都是 n 维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示。

点的变化对应着向量的线性变换（linear transformation），而描述对象变化抑或向量变换的数学语言，正是矩阵。而对坐标系施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘。

矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向。

### 概率论

概率的估计有两种方法：**最大似然估计法**（maximum likelihood estimation）和**最大后验概率法**（maximum a posteriori estimation），两者分别体现出频率学派和贝叶斯学派对概率的理解方式。

频率学派认为假设是客观存在且不会改变的，即存在**固定的先验分布**，只是作为观察者的我们无从知晓。

贝叶斯学派则认为**先验分布是随机**的，模型参数要靠后验概率最大化计算。

随机变量可以分成两类：**离散型随机变量**（discrete random variable）和**连续型随机变量**（continuous random variable）.

概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性之间的相对关系。

重要的离散分布包括两点分布、二项分布和泊松分布，重要的连续分布则包括均匀分布、指数分布和正态分布:

**两点分布**（Bernoulli distribution）：适用于随机试验的结果是二进制的情形，事件发生 / 不发生的概率分别为 p/(1−p)。任何只有两个结果的随机试验都可以用两点分布描述，抛掷一次硬币的结果就可以视为等概率的两点分布。

**二项分布**（Binomial distribution）：将满足参数为 p 的两点分布的随机试验独立重复 n 次，事件发生的次数即满足参数为 (n,p) 的二项分布。

**泊松分布**（Poisson distribution）：放射性物质在规定时间内释放出的粒子数所满足的分布。当二项分布中的 n 很大且 p 很小时，其概率值可以由参数为 λ=np 的泊松分布的概率值近似。

**均匀分布**（uniform distribution）：在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。

**指数分布**（exponential distribution）：满足参数为 θ 指数分布的随机变量只能取正值，指数分布的一个重要特征是无记忆性。

**正态分布**（normal distribution）：正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。

数字特征是用于刻画随机变量某些特性的常数，包括数学期望（expected value）、方差（variance）和协方差（covariance）。

### 数理统计

数理统计（mathematical statistics）的任务是根据可观察的样本反过来推断总体的规律；

推断的工具是统计量，统计量是样本的函数，是个随机变量；

统计推断的基本问题可以分为两大类：参数估计（estimation theory）和假设检验（hypothesis test）。

**参数估计**是通过随机抽取的样本来估计总体分布的方法，又可以进一步划分为**点估计（point estimation）和区间估计（interval estimation）**

**点估计**的具体方法包括矩估计法（method of moments）和最大似然估计法（maximum likelihood estimation）。**矩估计法**的思想在于用样本的 k 阶矩估计总体的 k 阶矩。**最大似然估计**：既然抽样得到的是已有的样本值，就可以认为取到这一组样本值的概率较大，因而在估计参数 θ 的时候就需要让已有样本值出现的可能性最大.

**估计量的评价**的三个基本标准。

- 无偏性：估计量的数学期望等于未知参数的真实值；
- 有效性：无偏估计量的方差尽可能小；
- 一致性：当样本容量趋近于无穷时，估计量依概率收敛于未知参数的真实值。

**假设检验通过随机抽取的样本来接受或拒绝关于总体的某个判断，常用于估计机器学习模型的泛化错误率。**

### 最优化方法

最优化问题是在无约束情况下求解给定目标函数的最小值；

在线性搜索中，确定寻找最小值时的搜索方向需要使用目标函数的一阶导数和二阶导数；

置信域算法的思想是先确定搜索步长，再确定搜索方向；

以人工神经网络为代表的启发式算法是另外一类重要的优化方法。

### 信息论

信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。

熵的本质:一个系统内在的混乱程度。

在机器学习中，信息增益常常被用于分类特征的选择

**最大熵原理**是确定随机变量统计特性时力图最符合客观情况的一种准则。对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值。

信息论处理的是客观世界中的不确定性；

条件熵和信息增益是分类问题中的重要参数；

KL 散度用于描述两个不同概率分布之间的差异；

最大熵原理是分类问题中的常用准则。

### 形式逻辑

常用的知识表示方法包括数据结构和处理算法。数据结构用于静态存储待解决的问题、问题的中间解答、问题的最终解答以及解答中涉及的知识；处理算法则用于在已有问题和知识之间进行动态交互，两者共同构成完整的知识表示体系。

亚里士多德提出并流传至今的三段论，它由两个前提和一个结论构成：科学是不断发展的；人工智能是科学；所以，人工智能是不断发展的。

在人工智能中应用的主要是一阶谓词逻辑。谓词逻辑是最基本的逻辑系统，也是形式逻辑的根本部分。谓词逻辑的一个特例是命题逻辑。在命题逻辑中，命题是逻辑处理的基本单位，只能对其真伪做出判断。

谓词逻辑既可以用于表示事物的概念、状态、属性等事实性知识，也可以用于表示事物间具有确定因果关系的规则性知识。

人工智能实现自动推理的基础是产生式系统。产生式系统包括规则库、事实库和推理机三个基本部分。

自动推理虽然在数学定理的证明上显示出强大的能力，可解决日常生活中的问题时却远远谈不上智能，其原因在于常识的缺失.

**第一不完备性定理**：在任何包含初等数论的形式系统中，都必定存在一个不可判定命题。

如果将认知过程定义为对符号的逻辑运算，人工智能的基础就是形式逻辑；

谓词逻辑是知识表示的主要方法；

基于谓词逻辑系统可以实现具有自动推理能力的人工智能；

不完备性定理向“认知的本质是计算”这一人工智能的基本理念提出挑战。

## 机器学习

### 机器学习概论

机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科。

**预测问题**可以分为以下三类。

- 分类问题：输出变量为有限个离散变量，当个数为 2 时即为最简单的二分类问题；
- 回归问题：输入变量和输出变量均为连续变量；
- 标注问题：输入变量和输出变量均为变量序列。

在机器学习中，误差被定义为学习器的实际预测输出与样本真实输出之间的差异。误差可以进一步分为训练误差和测试误差两类。测试误差则反映了学习器对未知的测试数据集的预测能力，是机器学习中的重要概念。

**过拟合**出现的原因通常是学习时模型包含的参数过多，从而导致训练误差较低但测试误差较高。

**过拟合**是机器学习中不可避免的问题，可通过选择合适的模型降低其影响；

**机器学习的任务**分成以下三类。

- 监督学习：基于已知类别的训练数据进行学习；效果较好
- 无监督学习：基于未知类别的训练数据进行学习；
- 半监督学习：同时使用已知类别和未知类别的训练数据进行学习。

根据学习方法的不同，监督学习可以分为**生成方法与判别方法**两类。

生成方法具有更快的收敛速度和更广的应用范围，判别方法则具有更高的准确率和更简单的使用方式。

### 线性回归

线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数。

最小二乘法可用于解决单变量线性回归问题，当误差函数服从正态分布时，它与最大似然估计等价；

对于单变量线性回归而言，在误差函数服从正态分布的情况下，从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的。

多元线性回归问题也可以用最小二乘法求解，但极易出现过拟合现象；

在线性回归中，正则化的方式根据其使用惩罚项的不同可以分为两种，分别是“岭回归”和“LASSO 回归”。

无论岭回归还是 LASSO 回归，其作用都是通过惩罚项的引入抑制过拟合现象，以训练误差的上升为代价，换取测试误差的下降.

岭回归和 LASSO 回归分别通过引入二范数惩罚项和一范数惩罚项抑制过拟合。

### 朴素贝叶斯方法

基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出。

朴素贝叶斯方法利用后验概率选择最佳分类，后验概率可以通过贝叶斯定理求解；

条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用。这样类条件概率就变成了属性条件概率的乘积.

属性的条件独立性假设是个相当强的假设。

从模型最优化的角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使**期望风险最小化**。

影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布。

### 逻辑回归

两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果。



逻辑回归模型是对线性回归的改进，用于解决分类问题；

逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；

在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果,除此之外，两者的区别还在于收敛速度的不同.但可以改进。

多分类问题时可以通过多次使用二分类逻辑回归或者使用 Softmax 回归解决。

### 决策树

决策树是包含根节点、内部节点和叶节点的树结构，通过判定不同属性的特征来解决分类问题；

决策树模型的学习过程包括三个步骤：特征选择、决策树生成和决策树剪枝。

决策树生成的基础是特征选择，特征选择的指标包括信息增益、信息增益比和基尼系数；

决策树的剪枝策略可以分为预剪枝和后剪枝。

### 支持向量机

支持向量机是一种二分类算法，通过在高维空间中构造超平面实现对样本的分类。

线性可分支持向量机就是在给定训练数据集的条件下，根据间隔最大化学习最优的划分超平面的过程。

线性支持向量机的通用性体现在将原始的硬间隔最大化策略转变为软间隔最大化。

不论是线性可分支持向量机还是线性支持向量机，都只能处理线性问题，对于非线性问题则无能为力。

将原始低维空间上的非线性问题转化为新的高维空间上的线性问题，这就是核技巧的基本思想。

核函数可以将线性支持向量机扩展为非线性支持向量机。

线性可分支持向量机通过硬间隔最大化求出划分超平面，解决线性分类问题；

线性支持向量机通过软间隔最大化求出划分超平面，解决线性分类问题；

非线性支持向量机利用核函数实现从低维原始空间到高维特征空间的转换，在高维空间上解决非线性分类问题；

支持向量机的学习是个凸二次规划问题，可以用 SMO 算法快速求解。

### 集成学习

集成学习正是使用多个个体学习器来获得比每个单独学习器更好的预测性能。

体学习器的生成方式很大程度上取决于数据的使用策略。根据训练数据使用方法的不同，集成学习方法可以分为两类：个体学习器间存在强依赖关系因而必须**串行生成的序列化方法**，和个体学习器之间不存在强依赖关系因而可以同时生成的**并行化方法**。

典型的序列化学习算法是**自适应提升方法**（Adaptive Boosting），人送绰号 AdaBoost。

AdaBoost 要解决两个主要问题：训练数据权重调整的策略和弱分类器结果的组合策略。

在训练数据的权重调整上，AdaBoost 采用专项整治的方式。

在 AdaBoost 的弱分类器组合中，每一轮得到的学习器结果都会按照一定比例叠加到前一轮的判决结果，并参与到下一轮次权重调整之后的学习器训练中。随着训练过程的深入，弱学习器的训练重心逐渐被自行调整到的分类器错误分类的样本上，因而每一轮次的模型都会根据之前轮次模型的表现结果进行调整，这也是 AdaBoost 的名字中“自适应”的来源。

AdaBoost 可以视为使用加法模型，以指数函数作为损失函数，使用前向分步算法的二分类学习方法。

典型的并行化学习方法是**随机森林方法**

知识要点：

集成学习使用多个个体学习器来获得比每个单独学习器更好的预测性能，包括序列化方法和并行化方法两类；

在随机森林中，每棵决策树在选择划分属性时，首先从结点的属性集合中随机抽取出包含 k 个属性的一个子集，再在这个子集中选择最优的划分属性生成决策树。

多样性要求集成学习中的不同个体学习器之间具有足够的差异性；

序列化方法采用 Boosting 机制，通过重复使用概率分布不同的训练数据实现集成，可以降低泛化误差中的偏差；

并行化方法采用 Bagging 机制，通过在训练数据中多次自助抽取不同的采样子集实现集成，可以降低泛化误差中的方差。

### 聚类分析

聚类分析是一种无监督学习方法，其目标是学习没有分类标记的训练样本，以揭示数据的内在性质和规律。

聚类分析是一种无监督学习方法，通过学习没有分类标记的训练样本发现数据的内在性质和规律；

数据之间的相似性通常用距离度量，类内差异应尽可能小，类间差异应尽可能大；

根据形成聚类方式的不同，聚类算法可以分为层次聚类、原型聚类、分布聚类、密度聚类等几类；

聚类分析的一个重要应用是对用户进行分组与归类。

### 降维学习

主成分分析是一种主要的降维方法，它利用正交变换将一组可能存在相关性的变量转换成一组线性无关的变量，这些线性无关的变量就是主成分

主成分分析遵循如下的步骤：

- 数据规范化：对 m 个样本的相同属性值求出算术平均数，再用原始数据减去平均数，得到规范化后的数据；
- 协方差矩阵计算：对规范化后的新样本计算不同属性之间的协方差矩阵，如果每个样本有 n 个属性，得到的协方差矩阵就是 n 维方阵；
- 特征值分解：求解协方差矩阵的特征值和特征向量，并将特征向量归一化为单位向量；
- 降维处理：将特征值按照降序排序，保留其中最大的 k 个，再将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵；
- 数据投影：将减去均值后的 m×n 维数据矩阵和由 k 个特征向量组成的 n×k 维特征向量矩阵相乘，得到的 m×k 维矩阵就是原始数据的投影。

主成分分析利用正交变换将可能存在相关性的原始属性转换成一组线性无关的新属性，并通过选择重要的新属性实现降维；

主成分分析的解满足最大方差和最小均方误差两类约束条件，因而具有最大可分性和最近重构性；

特征选择则是选取原始特征中的一个子集用于学习任务，是另一种主要的降维技术；

特征选择的关键问题是对特征子集的评价，主要的特征选择算法包括包裹法、过滤法和嵌入法。

## 人工神经网络 

### 神经网络的生理学背景

思维过程是神经元的连接活动过程，由大量突触相互动态联系着的众多神经元协同作用来实现；

大脑的思维源于从神经元到神经网络再到神经回路的功能逐级整合；

大脑对信息的加工可以理解为复杂的多次特征提取过程；

在大脑中，数据的传输和处理是同步进行的。

### 神经元与感知器

人工神经网络的神经元用传递函数对输入的线性加权进行非线性处理以产生输出；

感知器是一种二分类的监督学习算法，通过自适应调整权重解决线性分类问题；

感知器的神经元之间通过权重传递信息，权重的变化根据误差来进行调节；

感知器不能解决以异或为代表的线性不可分问题。

### 多层感知器

在感知器的输入层和输出层之间添加隐藏层，就可以得到多层感知器；

多层感知器是一类前馈神经网络，采用的是反向传播的学习方式；

反向传播算法通过求解误差函数关于每个权重系数的偏导数，以此使误差最小化来训练整个网络。

反向传播算法要根据误差函数的梯度来调整权重系数，需要应用求导的链式法则；

单个隐藏层就能使多层感知器以任意精度逼近任意复杂度的连续函数。

### 径向基函数神经网络

感受野指的是位于这一区域内的适当刺激能够引起该神经元反应的区域。按照感受野的变化规律设置权重系数，得到的就是“径向基函数神经网络”.

径向基网络通常包含三层：一个输入层、一个隐藏层和一个输出层。其中隐藏层是径向基网络的核心结构。

径向基网络采用局部逼近方式，每个神经元只对特定的输入信号产生作用；

径向基网络的隐藏神经元使用径向基函数作为传递函数，常用的径向基函数是高斯函数；

径向基函数可以将低维空间上的线性不可分问题转化为高维空间上的线性可分问题；

使用高斯函数的径向基网络可以用 K 均值聚类算法结合递归最小二乘法进行训练。

### 自组织特征映射

自组织映射是一类无监督学习的神经网络，模拟了生物神经系统的竞争性学习机制；

自组织映射能将任意维度的输入模式转换为一维或二维的离散映射，得到的特征映射是拓扑有序的；

在拓扑映射中，输出神经元的空间位置对应了输入数据的模式或特征；

自组织映射网络的训练包括竞争过程、合作过程和自适应过程等几个主要步骤。

- 竞争过程：对每个输入模式，网络中的神经元都计算其判别函数的取值，判别函数值最大的神经元成为竞争过程的优胜者；
- 合作过程：获胜神经元决定兴奋神经元的拓扑邻域的空间位置；
- 自适应过程：兴奋神经元适当调节其权重系数，以增加它们对于当前输入模式的判别函数值，强化未来对类似输入模式的响应。

自组织映射的训练算法可以归纳为以下几个步骤：

1. 使用主成分法或随机法初始化神经元的权重系数；
2. 选取训练集中的样本用于激活整个网络；
3. 根据最小距离准则寻找最佳匹配神经元；
4. 通过更新方程调整所有神经元的权重系数；
5. 重复以上步骤直到在从输入模式到神经元的映射关系中观察不到明显变化。

### 模糊神经网络

模糊神经网络是神经网络和模糊逻辑结合形成的混合智能系统；

模糊神经网络的输入信号、权重系数和输出信号全都是模糊集合；

模糊神经网络的主要学习算法包括基于水平集的方法和基于遗传算法的方法；

模糊神经网络具有和传统神经网络类似的通用逼近特性。

## 深度学习

### 深度学习概述

深度学习实际上是基于具有多个隐藏层的神经网络的学习；

深度学习的思想来源于人类处理视觉信息的方式；

深度学习的发展得益于数据的井喷和计算力的飙升；

深度学习的理论基础依然有待深入。

### 深度前馈网络

深度前馈网络利用深度架构实现工程上可实现的对任意函数的通用逼近；

深度前馈网络使用梯度下降的方法进行学习；

深度前馈网络的损失函数通常是交叉熵或最小均方误差；

深度前馈网络的隐藏神经元通常使用整流线性单元作为传递函数。

### 深度学习中的正则化

正则化被定义为对学习算法的修改，这些修改的目的在于减少泛化误差。

正则化处理可以看成是奥卡姆剃刀原则（Occam's razor）在学习算法上的应用。**奥卡姆剃刀原则**的表述是：“当两个假说具有完全相同的解释力和预测力时，以那个较为简单的假说作为讨论依据。”

基于训练数据的正则化方法包括数据集增强和 Dropout；

基于网络架构的正则化方法包括参数共享和传递函数正则化；

基于误差函数和正则化项的正则化方法包括使用 L2 范数和 L1 范数；

基于最优化过程的正则化方法包括早停。

**最优化过程的正则化**分为三种：对初始化（initialization）的正则化，对参数更新（weight update）的正则化，对终止条件（termination）的正则化。

### 深度学习中的优化

深度学习中的优化需要解决病态矩阵、局部极小值和鞍点等问题；

深度学习优化中的降噪方法包括动态采样、梯度聚合和迭代平均；

> 动态采样和梯度聚合两类方法是通过使用固定的步长来获得线性的收敛速度，进而实现降噪.迭代平均方法不是通过对梯度估计求平均，而是对每次迭代得到的参数结果求平均来实现降噪。

深度学习优化中的二阶导数近似方法是对原始牛顿法的各种改进；

其他优化方法包括动量方法、加速下降方法和坐标下降方法。

### 自编码器

自编码器是一种无监督学习方式，目的在于学习数据的重新表达；

由于种种隐式或显式正则化规则的限制，大权重系数是难以实现的，因而自编码器只能对数据进行重新的编码。

自编码器结构由编码映射和解码映射两部分组成。

如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合;

多个浅层自编码器级联可以得到深度的栈式自编码器，并使用无监督预训练结合有监督微调的方式加以训练；

栈式自编码器的训练策略可以归结为两个步骤：无监督预训练 + 有监督微调;

稀疏自编码器利用稀疏的高维表达提取出训练集中隐含的统计规律；

变分自编码器对隐藏层做参数化处理，可以用于学习数据的生成模型。

自编码器面对的一个问题是，对输入信号什么样的表达才能称为好的表达呢？同深度学习领域中的大多数问题一样，这个问题也不存在标准答案。从不同的角度回答它，得到的就是对原始自编码器的不同改进。前文中提到的过度完备的自编码器就是改进之一。由于在高维的隐藏层中，大部分神经元是被抑制的，只有少数能够输出特征表达，因而这类结构又被称为稀疏自编码器（sparse autoencoder）;

逆概率最大化问题可以转化为负对数似然的最小化，并利用基于梯度的方法求解。另一种训练方法是得分匹配，

### 深度强化学习

强化学习（reinforcement learning）实质上是智能系统从环境到行为的学习过程，智能体通过与环境的互动来改善自身的行为，改善准则是使某个累积奖励函数最大化。

描述强化学习最常用的模式是马尔可夫决策过程（Markov decision process）。马尔可夫决策过程是由离散时间随机控制的过程;

深度强化学习方法可以分成三类，分别是基于价值、基于策略和基于模型的深度强化学习。

策略梯度方法的思想是直接使用逼近函数来近似表示和优化策略，通过增加总奖励较高情况的出现概率来逼近最优策略。其运算方式和深度学习中的随机梯度下降法类似，都是在负梯度的方向上寻找最值，以优化深度网络的参数。

一种实用的策略梯度方法是无监督强化辅助学习（UNsupervised REinforcement and Auxiliary Learning），简称UNREAL 算法。UNREAL 算法的核心是行动者 - 评论家（actor-critic）机制，两者分别代表两个不同的网络。

基于模型（model-based）的深度强化学习的基本思路是构造关于环境的模型，再用这个模型来指导决策。

要点如下：

深度强化学习是深度学习和强化学习的结合，有望成为实现通用人工智能的关键技术；

基于价值的深度强化学习的基本思路是建立价值函数的表示，通过优化价值函数得到最优策略；

基于策略的深度强化学习的基本思路是直接搜索能够使未来奖励最大化的最优策略；

基于模型的深度强化学习的基本思路是构造关于环境的转移概率模型，再用这个模型指导策略。

## 神经网络

### 深度信念网络

深度信念网络是一种概率生成模型，能够建立输入数据和输出类别的联合概率分布。

受限玻尔兹曼机是构成深度信念网络的基本单元，是由可见层和隐藏层构成的神经网络；可见层用来接收数据，隐藏层则用来处理数据。可见层和隐藏层以全连接的方式相连，但同一层中的神经元则不会互相连接。

受限玻尔兹曼机的训练方法是对比散度法，通过可见层和隐藏层的多轮交互实现；

对比散度的训练过程本质上是求出一个最符合训练数据集统计特性的概率分布，也就是使训练数据集出现的概率最大的分布参数。

将几个受限玻尔兹曼机堆叠在一起，就可以得到深度信念网络（deep belief network）。

深度信念网络的无监督预训练也是逐层实现的。

深度神经网络的通用训练方式是无监督逐层预训练和有监督微调的结合。

传统的反向传播方法应用于深度结构在原则上是可行的，可实际操作中却无法解决梯度弥散（gradient vanishing）的问题，相比之下，基于预训练的训练方法就不会受梯度弥散的困扰。

### 卷积神经网络

卷积神经网络（convolutional neural network）指的是至少在某一层中用卷积运算（convolution）来代替矩阵乘法的神经网络。

卷积是对两个函数进行的一种数学运算，以核函数作为权重系数，对输入进行加权求和的过程。

卷积神经网络是应用了卷积运算的神经网络，适用于处理网格化数据；

卷积神经网络具有**稀疏感知性、参数共享性和平移不变性**；

- 稀疏感知性（sparse interaction）指的是卷积层核函数的大小通常远远小于图像的大小。
- 参数共享性（parameter sharing）指的则是在一个模型中使用相同的参数。
- 平移不变性（translational equivalence）指的是当卷积的输入产生平移时，其输出等于原始输出做出相同数量的平移，这说明平移操作和核函数的作用是可以交换的。

卷积神经网络的结构包括交替出现的**卷积层、激活层和池化层**，以及作为输出的全连接层；

常见的**最大池化**（max pooling）的做法就是将特征映射划分为若干个矩形区域，挑选每个区域中的最大值，也就是最明显的特征作为下采样的结果。

卷积神经网络的作用是逐层提取输入对象的特征。

### 循环神经网络

RNN这个缩写有两层含义，它既可以表示循环神经网络（Recurrent Neural Network），也可以表示递归神经网络（Recursive Neural Network）。循环神经网络可以看成是递归神经网络的特例，递归神经网络则可以视为循环神经网络的推广。

时间维度上的**参数共享**可以充分利用数据之间的**时域关联性**。

循环神经网络是具有记忆的神经网络，适用于处理序列化数据；

循环神经网络引入反馈结构，能够在时间上共享参数，从而具有记忆；

循环神经网络的扩展包括**双向循环网络和深度循环网络**；

其实前馈网络在某种程度上同样具有记忆，只要神经网络的参数经过最优化，优化的参数中就会包含着过往数据的踪迹。

两种网络代表了两种不同的知识类型:前馈网络适用于表示客观性的知识,循环网络则适用于表示主观性的知识。

循环神经网络训练方法也是基于梯度的反向传播算法，但和其他前馈网络不同的是，这里的反向传播是通过时间进行的;

由于循环神经网络的每个状态都与之前的所有状态相关，因而在基于时间的反向传播中，对当前时刻的参数求偏导一定会涉及前一时刻的参数。这其实和原始的反向传播算法毫无区别，只不过在链式法则中添加了一组关于时间的中间变量。

让当前的状态和以后时刻的状态同样建立起联系，得到的就是双向循环神经网络（bidirectional recurrent neural network），将深度结构引入双向循环神经网络就可以得到深度循环网。

递归神经网络能够处理具有层次化结构的数据，可以看成循环神经网络的推广。

### 生成式对抗网络

生成式对抗网络:这是一类在无监督学习中使用的人工智能算法，由两个在零和游戏框架下相互竞争的神经网络实现;由生成器和判别器构成；

由于生成器和判别器处于零和博弈之中，因而对网络的训练就可以等效成对以下目标函数的极大 - 极小问题;

其中“极大”是让判别器区分真实数据和伪造数据的准确率最大化，“极小”则是让生成器生成的数据被判别器发现的概率最小化。对整体极大 - 极小问题的优化可以通过交替迭代训练的方式实现。

生成式对抗网络好就好在摆脱了对模型分布的依赖，也不限制生成的维度，因而大大拓宽了生成数据样本的范围。

生成式对抗网络能够整合不同的损失函数，增加了设计的自由度;

缺点:最主要的一个问题就是缺乏理论基础;生成式对抗网络面临的另一个主要问题就是训练的难度

重点:
1,生成式对抗网络是一类运行在零和博弈框架下的无监督学习算法，由生成器和判别器构成；

2,生成器的目的是精确模拟真实数据的分布，判别器的目的是精确区分真实数据和生成数据；

3,生成式对抗网络的主要优点是超越了传统神经网络分类和特征提取的功能，能够按照真实数据的特点生成新的数据；

4,生成式对抗网络的主要问题是理论基础的缺失。

### 长短期记忆网络

**长短期记忆网络**就是一类特殊的循环神经网络。这个词的断句方式是“长 - 短期记忆网络”，表达的含义是一类可以持续很长时间的短期记忆模型。

循环神经网络通过在时间上共享参数引入了记忆特性，从而将先前的信息应用在当前的任务上，可这种记忆通常只有有限的深度;

从机制上讲，要实现长期记忆，神经网络既要学会记忆，也要学会遗忘。

长期记忆要求模型具备对信息价值的判断能力，结合自身的状态确定哪些信息应该保留，而哪些信息应该舍弃;

长短期记忆单元还要能够将长期记忆聚焦成工作记忆，也就是哪一部分记忆需要立刻使用。

长短期记忆的基本单元的作用在需要时取出并聚焦记忆，通常包括四个功能不同的隐藏层：记忆模块（memory cell）、输入门（input gate）、输出门（output gate）和遗忘门（forget gate），这比只有一个激活函数的一般循环神经网络要复杂得多。

遗忘门的作用是弃旧，输入门的作用则是图新。

输出门输出权重系数的作用是对记忆模块的状态进行加权。但加权对象不是记忆状态本身，而是记忆状态的双曲正切函数结果。

长短期记忆网络应用:谷歌翻译;

重点:
1,长短期记忆网络可以实现任意长度的记忆，对信息进行长期而精确的跟踪；

2,长短期记忆单元的组成包括记忆模块、输入门、遗忘门和输出门；

3,长短期记忆网络根据当前的输入、当前的记忆和前一时刻的输出确定当前的输出；

4,长短期记忆网络能够解决梯度弥散的问题

## 深度学习之外的人工智能

### 概率图模型

概率图模型（probabilistic graphical model）也叫结构化概率模型，是用图论表现随机变量之间的条件依赖关系的建模方法。典型的概率图模型包括**贝叶斯网络和马尔可夫随机场**，分别对应着**有向图模型和无向图模型**。

贝叶斯网络（Bayesian network）的拓扑结构是**有向无环图**

贝叶斯网络的作用是表示出随机变量之间的条件依赖关系，将多个随机变量的联合概率分布分解为条件概率因子乘积的形式

朴素贝叶斯分类的基础假设是不同的属性之间条件独立，因此类条件概率可以表示成属性条件概率的乘积。但在绝大多数情形下，这个假设是不成立的。将属性之间的依赖关系纳入后，得到的就是通用的贝叶斯网络。

贝叶斯网络的每个顶点只取决于有箭头指向它的其他顶点，而与没有箭头指向它的其他顶点条件独立;

贝叶斯网络能够根据各变量之间的条件依赖性，利用局部分布来求得所有随机变量的联合分布。

贝叶斯网络的构造包括两个步骤：首先要根据变量之间的依赖关系建立网络的拓扑结构，其次要根据拓扑结构计算每条边上的权重，也就是条件概率。

将贝叶斯网络的有向边替换为无向边，得到的就是马尔可夫随机场。马尔可夫随机场（Markov random field）属于无向图模型，它的每个顶点表示一个随机变量，每条边则表示随机变量之间的依赖关系;

马尔可夫随机场侧重于表示随机变量之间的相互作用;

概率模型三步框架:表示,推断,学习;

重点:
11,概率图模型是概率论与图论的结合，是用图论表现随机变量之间的条件依赖关系的建模方法；

12,贝叶斯网络是有向无环图，侧重于表示随机变量之间的依赖关系；

13,马尔可夫随机场是无向图，侧重于表示随机变量之间的相互作用；

14,概率图模型体现了“表示 - 推断 - 学习”的问题解决框架。

### 集群智能

集群智能的核心是由众多简单个体组成的群体能够通过相互之间的简单合作来实现某些功能，完成某些任务。

群体本身不具备中心化的结构，而是通过个体之间的识别与协同达成稳定的分布式结构。这个分布式结构会随着环境的变化，以自身为参考系不断趋于新的稳定。

集群智能（swarm intelligence）正是群居性生物通过协作表现出的自组织与分布式的宏观智能行为，它具有如下的特点：
A,第一个特点是可扩展性
B,第二个特点是并行性
C,第三个特点是容错性

用集群智能方法实现人工智能，代表的是研究方式的转变;

从结构模拟出发，通过人为创造类似人类脑神经系统的结构模型，实现智能的大规模涌现。

当构成一个系统的基本单元数量极为庞大时，将这些个体看作一个整体，就会有一些全新的属性、规律或模式自发地冒出来，这种现象就称为“涌现”（emergence）。

对涌现现象的代表性应用，就是以蚁群算法为代表的各类粒子群算法。

蚁群优化算法（ant colony optimization）：通过对蚁群在寻找食物过程中发现最优路径行为的模拟来寻找问题的最优解。

在行进的过程中，蚂蚁会在自己的路径上释放信息素，信息素的强度是与解的最优程度成正比的，新的蚂蚁则会根据已有信息素的强度选择自己的行进路径。

蚁群算法就是利用这种正反馈机制逐步遍历解空间，使搜索向最优解推进。

蚁群的行为与蚁群算法的诞生对人工智能的研究也是一种启示：相对于宏观的功能模拟，结构模拟重于微观的解构;

功能模拟是个自顶向下的过程，结构模拟则是个自底向上的过程；

重点:
1,集群智能是由众多简单个体通过自组织和去中心化的简单合作实现的智能；

2,集群智能具有可扩展性、并行性和容错性等特点；

3,集群智能体现出微观个体之间的相互作用能够实现整体大于部分之和的效果，其实例是蚁群算法；

4,集群智能在人工智能中的应用代表的是从宏观模仿到微观解构的方向转变。

### 迁移学习

迁移学习（transfer learning）是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”。

迁移学习可以看作是提升学习算法泛化性能的一种思路;

在迁移学习中，已有的知识（包括样本数据集及其分布）叫做源域，要学习的新知识叫做目标域。同样，待解决的任务也可以分为源任务和目标任务。

迁移学习问题划分三类:a,归纳迁移学习b,直推式迁移学习,c,无监督迁移学习

TrAdaBoost 是典型的基于样本的方法。TrAdaBoost 通过多次迭代的方式调整源域数据的权重，以鼓励“好”数据和抑制“坏”数据。

基于样本的迁移学习是对已有样本的重用过程，它通过调整源域中原始样本的权重系数，使之和目标域匹配，进而应用在目标域中;

基于特征的迁移学习是特征表示的重建过程，它通过特征变换使得源域数据与目标域数据处在同一个特征空间之上，再在这个公共空间上进行学习;

基于模型的迁移学习是已有模型的转移过程，它假设源任务和目标任务共享一些参数或者一些先验分布，将在训练数据上训练好的成熟模型应用到目标域上解决问题。

基于关系的迁移学习是问题结构的复制过程，如果源域和目标域之间共享了某种相似关系，那就可以将源域上的逻辑关系网络应用到目标域上;

每个特定的学习算法都建立在一系列有关数据的假设基础上，这叫作**归纳偏差**。只有当归纳偏差和学习问题相匹配时，算法才能获得良好的效果。

**迁移学习将会是个潮流**。

重点:
1,迁移学习是运用已学习的知识来求解不同但相关领域问题的新的机器学习方法，目的是让机器“学会学习”；

2,迁移学习适用于跨领域和小数据的学习任务；

3,迁移学习的任务类型可以分为归纳迁移学习，直推式迁移学习和无监督迁移学习；

4,迁移学习的学习方法包括基于样本、基于特征、基于模型和基于关系

### 知识图谱

深度学习的黑箱特性：虽然深度学习算法能够将图片中的猫咪辨识出来，却无法详细地解释为什么会做出这样的判断，其判定方法是否具备普适性也无从知晓。

人工智能的一个重要研究方向就是开发具有更好的可解释性，更容易被人理解的人工智能模型。

知识图谱（knowledge graph）是由大量的概念实体以及它们之间的关系共同构成的语义网络;

知识图谱可以根据已有实体的概念、属性和关系为新的概念自动生成属性；也可以明确不同新实体之间的关系;

归纳推理能够从旧知识中获取新知识，是知识的增殖过程。

数理逻辑能够实现的推理建立在硬性约束的基础上，只能实现非黑即白的推理过程，相比之下，知识图谱则可以实现软性推理。

路径排序算法的实现包括特征抽取、特征计算和分类器训练三个步骤，软性推理应用在演绎过程中，得到的就是马尔可夫逻辑网和概率软逻辑。

马尔可夫逻辑网（Markov logic network）是将无向概率图模型和一阶谓词逻辑相结合得到的统计关系学习模型;

对马尔可夫逻辑网加以扩展，给网络中每个顶点所代表的原子事实赋予一个连续分布的真值，得到的就是概率软逻辑。概率软逻辑能够对不确定的事实和规则同时建模，因而具有更强的不确定性处理能力。

分布式知识表示（knowledge graph embedding）是将包含实体和关系的知识图谱组件嵌入到连续的向量空间中，以便在保持知识图谱内在结构的同时简化操作;

重点:

1,知识图谱是由大量的概念实体以及它们之间的关系构成的语义网络；

2,用知识图谱实现从特殊到一般的归纳推理，典型的方法是路径排序算法；

3,用知识图谱实现从一般到特殊的演绎推理，典型的方法是马尔可夫逻辑网和概率软逻辑；

4,用知识图谱实现数值推理，典型的方法是基于分布式知识表示的方法。

## 应用场景

### 计算机视觉

传统的计算机视觉方法通常包括图像预处理、特征提取、特征筛选、图像识别等几个步骤

深度残差网络:残差（residual）是残差网络的核心元素，但这个概念却并不复杂。没有引入残差的普通网络将输入 x 映射为 H(x)，训练的意义就在于使用大量训练数据来拟合出映射关系 H(x)。可残差网络独辟蹊径，它所拟合的对象并不是完整的映射 H(x)，而是映射结果与输入之间的残差函数 F(x)=H(x)−x。换句话说，整个网络只需要学习输入和输出之间差异的部分，这就是残差网络的核心思想。

残差能够带来优良的效果:因为残差网络在一定程度上解决了深度结构训练难的问题，降低了优化的难度

密集连接卷积网络:指的是网络中的任意两层都有直接的连接，每个层的输入都是之前所有层输出的集合。这样一来，每个层次都要处理所有提取出来的低层与高层特征;

重点:
1,在传统的计算机视觉方法中，特征设计和分类器训练是割裂的；

2,以卷积神经网络为代表的深度结构可以实现通用的物体识别算法；

3,深度残差网络将输出和输入之间的残差作为拟合对象，解决了深度神经网络训练难的问题；

4,密集连接网络采用全连接方式，实现了特征的高度重用，降低了参数数量和训练难度。

### 语音处理

语音处理包括语音识别和语音合成两部分;

业界主流的语音合成方法有两种：单元选择和参数合成;

Siri 的语音合成系统包括文本分析、音韵生成、单元选择、波形串联四个模块，前两个环节对应前端的文本处理，后两个环节则对应后端的信号处理;

对于每个目标半音素，维特比算法都可以搜索出一个最优单元序列来合成它，评价最优性的指标包括两条：目标成本和拼接成本。

Siri 的独特之处在于将深度学习应用在了混合单元选择模式中：用基于深度学习的一体化模型代替传统的隐马尔可夫模型指导最优单元序列的搜索，以自动并准确地预测数据库中单元的目标损失和拼接损失;

Siri 使用的技术是深度混合密度网络（Mixture Density Network），这是传统的深度神经网络和高斯混合模型（Gaussian Mixture Model）的组合。

语音识别能够将语音信号转换成对应的文本信息，其系统通常包含预处理、特征提取、声学模型，语言模型和字典解码等几个模块。

隐马尔可夫模型可以通过隐藏节点的引入解决特征序列到多个语音基本单元之间的对应关系，也使用期望最大化算法加以训练。

与隐马尔可夫模型相比，神经网络的优点在于不依赖对特征统计特性的任何假设，但其缺点则是对时间上的依赖关系的建模能力较差，因而缺乏处理连续识别任务的能力。

**Siri 在声学模型的训练中用到了迁移学习技术**，通过跨带宽和跨语言的初始化来提升神经网络的声学模型。

不同语言、不同带宽语音数据的神经网络训练可以在同样的框架下进行，其基础是神经网络中特征变换的泛化特性，这使得特征变换的方法不依赖于具体的语言。

重点:
1,语音处理可以分为语音识别和语音合成两类任务；

2,语音合成过程包括文本分析、音韵生成、单元选择、波形串联等步骤；

3,语音识别过程包括预处理、特征提取、声学模型，语言模型和字典解码等步骤；

4,深度学习和迁移学习等技术都已经被应用在语音处理之中。

### 对话系统

人工智能的一个基本挑战就是赋予机器使用自然语言与人交流的能力。

基本技术问题总结为以下五个：重要词语的识别，最小语境范围的判定，恰当的转化选择，适当回复的生成和结束对话的能力。

基于神经网络的端到端对话系统不需要人为介入，而是从对话本身中进行学习;

端到端对话系统采用的模型是记忆网络。相对于普通神经网络，记忆网络的优势在于能够实现长期记忆，对话中的每一句话都被存储到记忆模块中，保证了信息不在压缩的过程中被丢失。

Facebook 提出了通过与人类对话者的线上互动实现学习的想法。

监督的目的不是纠正某个单独语句的错误，而是从策略上动态改善对话表现。

在互动中，聊天机器人不仅要回答问题，更要提出问题，并从提问中进行学习。机器人对何时提问和提问什么要有一定的判断力，而这种判断力可以通过强化学习来建模和训练。

理想的提问策略是机器人首先学习对话任务，再根据基于提问成本的问答策略和自身回答问题的能力来学习改善自己的性能。

重点:
1,早期的对话系统通过模式匹配和智能短语搜索对人类的合适回复；

2,智能个人助理可以帮助用户在多个垂直领域完成任务；

3,社交聊天机器人的作用是满足用户的情感需求；

4,神经网络能够帮助社交聊天机器人实现通用化的学习。

> 1,<<流浪地球>>中的机器人:
> 理智对人工智能来说是非常容易的，但是对人来说总是不那么容易
> 而人类也正是无法永远保持理智，有了自己的情感才被称为理智
> 2,<<庆余年>>中的神庙四定律:0,“第零定律。神庙必须保护人类的整体利益不受伤害，其它三条定律都是在这一前提下才能成立;
> “第一定律，神庙不得伤害人类，也不得见人类受到伤害而袖手旁观。第二定律。神庙应服从人类地一切命令，但不得违反第一定律。第三定律，神庙应保护自身的安全，但不得违反第一、第二定律……
> 3,阿西莫夫的《我，机器人》:机器人学的三大法则,第一定律：机器人不得伤害人类个体，或者目睹人类个体将遭受危险而袖手不管;第二定律：机器人必须服从人给予它的命令，当该命令与第一定律冲突时例外;第三定律：机器人在不违反第一、第二定律的情况下要尽可能保护自己的生存

### 机器翻译

1949 年，洛克菲勒基金会的科学家沃伦·韦弗提出了利用计算机实现不同语言的自动翻译的想法，并且得到了学术界和产业界的广泛支持;

语言作为信息的载体，其本质可以被视为一套编码与解码系统，只不过这套系统的作用对象是客观世界与人类社会。将字 / 词看成构成语言的基本元素的话，每一种语言就都可以解构为所有字 / 词组成的集合。而引入中介语言可以把所有语言的编码统一成为用于机器翻译的中间层，进而实现翻译。

基于深度学习和海量数据的统计机器翻译已是业界主流;

机器翻译的实现理念从句法结构与语序特点的规则化解构转换为对大量平行语料的统计分析构建模型，曙光才出现在地平线上。

谷歌的神经机器翻译最主要的特点是整体处理，也就是将整个句子视作翻译单元。在结构上，神经机器翻译建立了由长短期记忆层构成了分别用于编码和译码的递归神经网络，并引入了注意力机制和残差连接，让翻译的速度和准确度都能达到用户的要求。在输出端，谷歌采用波束搜索技术来选取最优的输出序列，并使用覆盖率惩罚和长度正则化来优化搜索过程。出于效率的考虑，神经机器翻译同时使用了数据并行计算和模型并行计算。

一个神经网络以任何语言作为输入并转换成任何输出语言，而不需要任意输入 - 输出语言之间的两两配对;换言之，谷歌实现了一把解锁不同语言的万能钥匙，这一通用的解决方案对机器翻译而言无疑具有里程碑式的意义。

零知识翻译的实现要归功于神经网络的迁移学习特性;

语言和文字本身就可以视为对客观世界进行编码的系统;

重点:

1,早期的机器翻译采用的是逐字对应的方法；

2,语言学的进展使机器翻译转而依赖句法规则；

3,谷歌将神经网络引入机器翻译之中，利用大量数据提升翻译精确性；

4,神经网络可以通过迁移学习“桥接”不同的语言，实现零知识翻译。



## 其他

### 数学基础

#### 机器学习三步走

建立抽象模型

- K- 近邻算法
- 回归模型
- 决策树
- SVM 支持向量机

评价模型

- 设定目标函数
- 最小均方误差和最大后验概率

优化

#### 数学工具

**线性代数**

主要包括这样两个部分：一方面是线性空间理论，也就是我们说的向量、矩阵、变换这样一些问题；第二个是矩阵分析。

线性代数，它就等于数量和结构的组合。它的作用，一方面可以把具体的事物抽象成数学对象，另外一方面，可以提升大规模运算的效率。；

**概率统计**

概率统计包括了两个方面，一方面是数理统计，另外一方面是概率论。

概率统计等于模型和数据的一个组合，这个组合是双向的。在学习阶段，我们利用数据来训练模型，在预测阶段，我们利用模型反过来去推断这个数据。

**最优化理论**

凸优化，无约束优化，反向传播

最优化=目标+约束

#### 掌握程度

**能使用**

给定一个模型，我能够用它来根据给定的输入来求解输出，也就是利用已知的方法来解决问题

**能看懂**

能够理解这个方法的工作原理

**能设计**

可以根据我的问题，根据我自己的实际问题的特点，来开发一些新的方法

#### 学习路径

掌握核心概念及其区别

以点带面：从关键问题去铺开

问题导向