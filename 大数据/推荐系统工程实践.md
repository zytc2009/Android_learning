[Toc]

## 常见架构

### 信息流架构

要搜索 feed 相关的技术文章，你应该用“Activity Stream”作为关键词去搜，而不应该只用“feed”搜索

要实现一个信息流，可以划分为两个子问题。

1. 如何实现一个按照时间顺序排序的信息流系统？
2. 如何给信息流内容按照兴趣重排序？

整体架构：

![](images\信息流架构.png)

主要模块：日志收集、内容发布、机器学习、信息流服务、监控

#### 数据模型

信息流的基本数据有三个：用户（User）、内容（Activity）和关系（Connection）。

##### 1. 内容即 Activity。

用于表达 Activity 的元素有相应的规范，叫作 Atom。根据 Atom 规范的定义，一条 Activity 包含的元素有：Time、Actor、Verb、Object、Target、Title、Summary。

Actor：即“Activity 由谁发出的”，也就是和用户建立连接的另一端。

Verb：动词，就是连接的名字，比如“Follow”“Like”等，也可以是隐含的连接

Object：即动作作用到最主要的对象，只能有一个

Target：动作的最终目标，与 verb 有关，可以没有

举个例子： 2016 年 5 月 6 日 23:51:01（Time）你（Actor） 分享了（Verb） 一条动态（Object） 给 好友 （Target）。把前面这句话去掉括号后的内容就是它的 Title，Summary 暂略。

除了上面的字段外，还有一个隐藏的 ID，用于唯一标识一个 Activity

##### 2. 关系即连接。

定义一个连接的元素有下面几种。

- From：连接的发起方。
- To：被连接方。
- Type/Name：就是 Atom 模型中的 Verb，即连接的类型：关注、加好友、点赞、浏览、评论，等等。
- Affinity：连接的强弱。

#### 动态发布

**拉”模式**（Fan-out-on-load）

不足：

- 随着连接数的增加，这个操作的复杂度指数级增加；
- 内存中要保留每个人产生的内容；
- 服务很难做到高可用。

**“推”模式**（Fan-out-on-write）

不足：

- 大量的写操作：每一个粉丝都要写一次。
- 大量的冗余存储：每一条内容都要存储 N 份（受众数量）。
- 非实时：一条内容产生后，有一定的延迟才会到达受众信息流中。
- 无法解决新用户的信息流产生问题。

简单的结合方案：活跃度高的用户，使用推模式，活跃度没有那么高的用户，使用拉模式

Etsy 的设计方案：受众用户与内容产生用户之间的亲密度高，则优先推送，反之则推迟推送或者不推送。也不是完全遵循亲密度顺序，而是采用与之相关的概率。

对于信息流的产生和存储可以选择的工具有：

- 用户信息流的存储可以采用 Redis 等 KV 数据库, 使用 uid 作为 key。
- 信息流推送的任务队列可以采用 Celery 等成熟框架。

### 信息流排序

定义好互动行为，区分好正向互动和负向互动，典型的二分类监督学习。

能产生概率输出的二分类算法都可以用在这里，比如贝叶斯、最大熵、逻辑回归等。

建议小厂或者刚起步的信息流先采用线性模型。

对于线性模型，一个重要的工作就是特征工程。信息流的特征有三类：

- 用户特征，包括用户人口统计学属性、用户兴趣标签、活跃程度等；
- 内容特征，一条内容本身可以根据其属性提取文本、图像、音频等特征，并且可以利用主题模型提取更抽象的特征。
- 其他特征，比如刷新时间、所处页面等。

### 数据管道

信息流是一个数据驱动的系统，既要通过历史数据来寻找算法的最优参数，又要通过新的数据验证排序效果

管道中要使用的相关数据可能有：

1. 互动行为数据，用于记录每一个用户在信息流上的反馈行为；
2. 曝光内容，每一条曝光要有唯一的 ID，曝光的内容仅记录 ID 即可；
3. 互动行为与曝光的映射关系，每条互动数据要对应到一条曝光数据；
4. 用户画像内容，即用户画像，提供用户特征；
5. 信息流的内容分析数据，提供内容特征，即物品画像。



从零开始的信息流，没必要做到在线实时更新排序算法的参数。数据的管道可以分成三块：生成训练样本，可离线；排序模型训练，可离线；模型服务化，实时服务；



Facebook和头条的feed推荐区别：

共同点：
1. Facebook和今日头条都是要通过内容提取，用户和环境的分析找到最匹配的信息；
2. 根据用户的各种行为来衡量效果；
3. 都会引入一些无法完全用数据衡量的目标。比如屏蔽广告，屏蔽骚扰帐号，屏蔽有害内容；
4. 特征提取，特征匹配，用各种机器学习和深度学习的模型。用户标签/画像，内容标签的建立。这些工作后面的机制是相通的；
5. 实时信息流的更新量大，对性能要求高；
6. 都会有实验平台和长期跟踪效果的记录平台；
7. 都有人工介入评估。

不同点：

1. Facebook里原创和转发的动作比今日头条更频繁(我的理解)，这个动作的衡量会不同；
2. 今日头条的内容更复杂，种类更丰富，需要提取的特征种类，特征信息和衡量效果的因素更多；
3. 今日头条的内容是有层级逻辑关系的；
4. Facebook人之间的关系，互动的影响要比今日头条之间要大；
5. 今日头条内容团队的中国特色。评估效果时人工介入的更多。

### 架构的重要性

好的推荐系统架构应该具有这些特质：

1. 实时响应请求；

2. 及时、准确、全面记录用户反馈；

3. 可以优雅降级；

4. 快速实验多种策略。

   

Netflix 的推荐系统架构图：

![](images\Netflix的推荐系统架构图.png)

一共分成三层：

离线：不用实时数据，不提供实时服务；

近线：使用实时数据，不保证实时服务；

在线：使用实时数据，要保证实时服务。

#### 1. 数据流

行为事件数据，实时地被收集走，一边进入分布式的文件系统中存储，供离线阶段使用，另一边流向近线层的消息队列，供近线阶段的流计算使用。

#### 2. 在线层

哪些计算逻辑适合放在在线层呢？

简单的算法逻辑；模型的预测阶段；商业目标相关的过滤或者调权逻辑；场景有关的一些逻辑；互动性强的一些算法。

#### 3. 离线层

批量、周期性地执行一些计算任务。

![](images\离线层的示意图.png)



在 Netflix内部，Hermes管理数据源，要求：

1. 数据准备好之后及时通知相关方，也就是要有订阅发布的模式；
2. 能够满足下游不同的存储系统；
3. 完整的监控体系，并且监控过程对于数据使用方是透明的。

离线阶段的任务主要是两类：模型训练和推荐结果计算。

通常机器学习类模型，尤其是监督学习和非监督学习，都需要大量的数据和多次迭代，这类型的模型训练任务最适合放在离线阶段。

大多数推荐算法，实际上都是在离线阶段产生推荐结果的。离线阶段的推荐计算和模型训练，如果要用分布式框架，通常可以选择 Spark 等。

#### 4. 近线层

它结合了离线层和在线层的好处，摒弃了两者的不足。也叫做准实时层

近线计算任务一个核心的组件就是流计算，因为它要处理的实时数据流。常用的流计算框架有 Storm，Spark Streaming，FLink 等，Netflix 采用的内部流计算框架 Manhattan，这和 Storm 类似。

对比结果：

![](images\推荐架构三层对比.png)

### 简化架构

要从 0 开始搭建一个推荐系统，那么可以以 Netflix 的架构作为蓝本，做一定的简化。

![](images\简化架构.png)

关键简化有两点：

完全舍弃掉近线层；

避免使用分布式系统。

### 三种信息获取方式

搜索引擎，推荐系统，广告系统

![](images\信息获取方式.png)

### 架构抽象

我们抽象一下三者的需求共性：本质上都是在匹配，匹配用户的兴趣和需求（看成 context），但匹配的目标，条件和策略不尽相同。

抽象下去，又可以分为三步：过滤候选、排序候选、个性化输出。

#### 1. 过滤候选

#### 2.排序候选

候选排序这一步，对于三者来说，主要区别在于排序的目标和约束。

#### 3. 个性化输出

#### 4. 三者的协同



## 推荐系统常见模块

### 日志和数据

数据采集，按照用途分类又有三种：报表统计；数据分析；机器学习

#### 数据采集

##### 1. 数据模型

数据模型，其实就是把数据归类。数据按矩阵分成了四种：

![](images\数据矩阵.png)

要收集的数据：

![](images\数据分类.png)

##### 2.数据来源

1.用户原始信息：注册信息

2.埋点数据：

   SDK 埋点：友盟等

   可视化埋点：嵌入可视化埋点套件的 SDK

   无埋点

按照收集数据的位置又分为前端埋点和后端埋点

##### 3.数据元素

用户 ID，物品ID，事件名称， 事件时间

其他：

设备信息，位置信息，来源页面，发生页面，用户属性，物品属性等

##### 4.数据收集

![](images\数据收集.png)

日志收集后，不论是 Logstash 还是 Flume，都会发送到 Kafka。在 Kafka 后端一般是一个流计算框架，上面有不同的计算任务去消费 Kafka 的数据 Topic，流计算框架实时地处理完采集到的数据，会送往分布式的文件系统中永久存储，一般是 HDFS。

##### 5. 质量检验

是否完整，是否一致，是否正确，是否及时

### 实时推荐

实时推荐，有三个层次：

第一层，“给得及时”，也就是服务的实时响应

第二层，“用得及时”，就是特征的实时更新

第三层，“改得及时”，就是模型的实时更新

#### 1. 架构概览

数据实时进来，数据实时计算，结果实时更新

![](images\实时推荐框图.png)

#### 2. 实时数据

Kafka 已经是非常成熟的选项

#### 3. 流计算

整个实时推荐建立在流计算平台上。常见的流计算平台有 Twitter 开源的 Storm，“Yahoo！”开源的 S4，还有 Spark 中的 Streaming。

相比 Streaming 的 MiniBatch 模式，Storm 才是真正的流计算，在你构建自己的实时推荐时，流计算平台不妨就选用 Storm，不过最新的流计算框架 FLink 表现强劲，如果有兴趣也可以试试。

Storm 是一个流计算框架，它有以下几个元素：

1. Spout，水龙头，接入一个数据流，然后以喷嘴的形式把数据喷洒出去。
2. Bolt，意思是螺栓，两端可以接入喷嘴，也可以接入另一个螺栓，数据流就进入了下一个处理环节。
3. Tuple，意思是元组，就是流在水管中的水。
4. Topology，意思是拓扑结构，螺栓和喷嘴，以及之间的数据水管，一起组成了一个有向无环图，这就是一个拓扑结构。

Storm 中要运行实时推荐系统的所有计算和统计任务，比如有下面几种：

清洗数据；合并用户的历史行为；重新更新物品相似度；在线更新机器学习模型；更新推荐结果。

#### 4. 算法实时化

以基于物品的协同过滤算法为主线，来讲解一下如何实现实时推荐

首先就是要做到增量更新物品之间的相似度，相似度计算分三部分：物品1的评价用户数、物品2的评价用户数、物品对的共同评价用户数。

转换成 Storm 的编程模型，你需要实现：

1. Spout：消费实时消息队列中的用户评分事件数据，并发射成（UserID , ItemID_i）这样的 Tuple
2. Bolt1：接的是源头 Spout，输入了 UserID 和 ItemID_i，读出用户历史评分 Item 列表，遍历这些 ItemID_j，逐一发射成 ((Item_i, Item_j), 1) 和 ((Item_j, Item_i), 1)，并将 Item_i 加进历史评分列表中；
3. Bolt2：接的是源头 Spout，输入了 UserID 和 ItemID_i，发射成 (ItemID_i, 1)；
4. Bolt3：接 Bolt1，更新相似度所需的分子
5. Bolt4：接 Bolt2，更新物品自己的评分用户数

另外，还有实时更新推荐结果，也是作为 Storm 的一个 Bolt 存在，接到用户行为数据，重新更新推荐结果，写回推荐结果数据中。

#### 5. 效率提升

应对办法：剪枝，加窗，采样，缓存。

剪枝：Hoeffding 不等式适用于有界的随机变量，计算更新次数和相似度关系，如果次数较少，相似度小于阈值，不参与计算

加窗：滑窗。设定一个时间窗口，时间窗口内的历史行为数据参与实时计算，窗口外的不再参与实时计算

采样： 手段有很多，可以均匀采样，也可以加权采样

合并计算：不必每一条来了都去更新，而是可以在数据流的上游做一定的合并。

缓存：实时更新的推荐结果同步到推荐服务所依赖的线上数据库，这个线上数据库还要定期被线下离线批量的推荐结果所替代。这样一来，实时推荐和离线批量之间就形成了互为补充的作用，这个模式也就是大数据架构最常见的 Lambda 架构。

### 数据驱动和实验平台

互联网实验，需要三个要素。

1. 流量：流量就是用户的访问，也是实验的样本来源。
2. 参数：参数就是各种组合，也是用户访问后，从触发互联网产品这个大函数，到最后返回结果给用户，中间所走的路径。
3. 结果：实验的全过程都有日志记录，通过这些日志才能分析出实验结果，是否成功，是否显著。

实验设计人员需要考虑下面这些问题：

1. 实验的起止时间。这涉及到样本的数量，关系到统计效果的显著性，也涉及能否取出时间因素的影响。
2. 实验的流量大小。这也涉及了样本的数量，关系到统计效果的显著性。流量的分配方式。每一个流量在为其选择参数分支时，希望是不带任何偏见的，也就是均匀采样，通常按照 UUID 或者 Cookie 随机取样。
3. 流量的分配条件。还有一些实验需要针对某个流量的子集，例如只对重庆地区的用户测试，推荐时要不要把火锅做额外的提升加权。
4. 流量如何无偏置。这是流量分配最大的问题，也是最难的问题。

#### 重叠实验架构

三个概念。

域：是流量的一个大的划分，最上层的流量进来时首先是划分域。

层：是系统参数的一个子集，一层实验是对一个参数子集的测试。

桶：实验组和对照组就在这些桶中。

层和域可以互相嵌套。

下面这两个图示意了有域划分和没有域划分的两种情况。

![](images\层和域嵌套.png)

为什么多层实验能做到重叠而不带来流量偏置呢？

这就需要说桶的概念。还是上面示意图中的左图，假如这个实验平台每一层都是均匀随机分成 5 个桶，在实际的实验平台上，可能是上千个桶。

示意图如下：

![](images\桶.png)

关于分层实验，有几点需要注意：

1. 每一层分桶时，不是只对 Cookie 或者 UUID 散列取模，而是加上了层 ID，是为了让层和层之间分桶相互独立；
2. Cookie 或者 UUID 散列成整数时，考虑用均匀的散列算法，如 MD5。
3. 取模要一致，为了用户体验，虽然是分桶实验，但是同一个用户在同一个位置每次感受不一致，会有损用户体验。

Google 的重叠实验架构还有一个特殊的实验层，叫做发布层，优先于所有其他的实验层，它拥有全部流量。这个层中的实验，通常是已经通过了 ABtest 准备全量发布了

四种流量分配方式：

1. Cookie+ 层 ID 取模；
2. 完全随机；
3. 用户 ID+ 层 ID 取模；
4. Cookie+ 日期取模。

在实验中，得到流量后还可以增加流量条件，比如按照流量地域，决定要不要对其实验，如果不符合条件，则这个流量不会再参与后面的实验，这样避免引入偏置，那么这个流量会被回收，也就是使用默认参数返回结果。

Google 的架构：

![](images\Google重叠实验架构.png)

说明如下：

1. 图中涉及了判断的地方，虚线表示判断为假，实线表示判断为真。
2. 从最顶端开始，不断遍历域、层、桶，最终输出一个队列 Re，其中记录对每一个系统参数子集如何处理，取实验配置的参数还是使用默认参数，其中无偏流量表示使用默认参数，也就是在那一层不参与实验，流量被回收。
3. 拿到 Re 就得到了全部的实验，在去调用对应的服务。

#### 统计效果

#### 对比实验的弊端

1. 落入实验组的流量，在实验期间，可能要冒着一定的风险得到不好的用户体验，在实验结束之前，这部分流量以 100% 的概率面对这不确定性；
2. 要得得到较高统计功效的话，就需要较长时间的测试，如果急于看到结果全面上线来说有点不能接收；
3. 下线的实验组如果不被人想起，就不再有机会得到测试。

可以考虑在实验平台中用 Bandit 算法替代流量划分的方式，通过 Bandit 算法选择不同的参数组合、策略，动态实时地根据用户表现给出选择策略，一定程度上可以避免上面列举的弊端。



### 存储

这里讲到的存储，专指近线或者在线部分所用的数据库，并不包括离线分析时所涉及的业务数据库或者日志数据库。

离线阶段会产生哪些数据。按照用途来分，归纳起来一共就有三类。

1. 特征。特征数据会是最多的，所谓用户画像，物品画像，这些都是特征数据，更新并不频繁。
2. 模型。尤其是机器学习模型，这类数据的特点是它们大都是键值对，更新比较频繁。
3. 结果。就是一些推荐方法在离线阶段批量计算出推荐结果后，供最后融合时召回使用。任何一个数据都可以直接做推荐结果，如协同过滤结果。



特征数据有两种，一种是稀疏的，一种是稠密的。

特征数据又常常要以两种形态存在：一种是正排，一种是倒排。正排就是以用户 ID 或者物品 ID 作为主键查询，倒排则是以特征作为主键查询。

这两种形态的特征数据，需要用不同的数据库存储。正排需要用列式数据库存储，倒排索引需要用 KV 数据库存储。前者最典型的就是 HBase 和 Cassandra，后者就是 Redis 或 Memcached。

另外，对于稠密特征向量，例如各种隐因子向量，Embedding 向量，可以考虑文件存储，采用内存映射的方式，会更加高效地读取和使用。

模型数据又分为机器学习模型和非机器学习模型。

机器学习模型与预测函数紧密相关。模型训练阶段，如果是超大规模的参数数量，业界一般采用分布式参数服务器，一般大公司才用。中小公司可以采用更加灵活的 PMML 文件作为模型的存储方式

最后，是预先计算出来的推荐结果，或者叫候选集，这类数据通常是 ID 类，召回方式是用户 ID 和策略算法名称。这种列表类的数据一般也是采用高效的 KV 数据库存储，如 Redis。

另外，还要介绍一个特殊的数据存储工具：ElasticSearch。这原本是一个构建在开源搜索引擎 Lucene 基础上的分布式搜索引擎，也常用于日志存储和分析，但由于它良好的接口设计，扩展性和尚可的性能，也常常被采用来做推荐系统的简单第一版，直接承担了存储和计算的任务。

#### 1. 列式数据库

列式数据库有个列族的概念，可以对应于关系型数据库中的表，还有一个键空间的概念，对应于关系型数据库中的数据库。

列式数据库适合批量写入和批量查询，因此常常在推荐系统中有广泛应用。列式数据库当推 Cassandra 和 HBase，两者都受 Google 的 BigTable 影响，但区别是：Cassandra 是一个去中心化的分布式数据库，而 HBase 则是一个有 Master 节点的分布式存储。

Cassandra 在数据库的 CAP 理论中可以平滑权衡，而 HBase 则是强一致性，并且 Cassandra 读写性能优于 HBase，因此 Cassandra 更适合推荐系统

Cassandra 的数据模型组织形式如下图所示：

![](images\Cassandra 的数据模型.png)

可以通过行主键及列名就可以访问到数据矩阵的单元格值

#### 2. 键值数据库

键值对内存数据库，首推 Redis。

Redis 你可以简单理解成是一个网络版的 HashMap，但是它存储的值类型比较丰富，有字符串、列表、有序列表、集合、二进制位。并且，Redis 的数据放在了内存中，快速读取。

在推荐系统的以下场景中常常见到 Redis 的身影：

1. 消息队列，List 类型的存储可以满足这一需求；
2. 优先队列，比如兴趣排序后的信息流，或者相关物品，对此 sorted set 类型的存储可以满足这一需求；
3. 模型参数，这是典型的键值对来满足。

Redis的问题就是不太高可用

#### 3. 非数据库

非主流但常用的存储方式。第一个就是虚拟内存映射，称为 MMAP，另外一个就是 PMML 文件，专门用于保存数据挖掘和部分机器学习模型参数及决策函数的。

### API

API 有两大类，一类数据录入，另一类是推荐服务

数据录入 API，可以用于数据采集的埋点，或者其他数据录入。

推荐服务的 API 按照推荐场景来设计：

#### 1. 猜你喜欢

接口：/Recommend

输入：用户信息，页面id，size，offset，position

输出：推荐列表，推荐id，size，page

#### 2. 相关推荐

接口：/Relative

输入：用户信息，页面id，itemId，size，offset，position

输出：推荐列表，推荐id，size，page

#### 3. 热门排行榜

接口：/Relative

输入：用户信息，页面id，size，offset，position

输出：推荐列表，推荐id，size，page

## 效果保证

### 测试方法

#### 1. 业务规则扫描

首先，业务规则扫描本质上就是传统软件的功能测试。

除了业务规则，数学计算中也有一些潜在的规则不能违反。

#### 2. 离线模拟测试

通常做法是先收集业务数据，也就是根据业务场景特点，构造用户访问推荐接口的参数。

利用历史真实日志构造用户访问参数，得到带评测接口的结果日志后，结合对应的真实反馈，可以定性评测效果对比。

#### 3. 在线对比测试

分流量做真实的评测。

#### 4. 用户访谈

对用户访谈，更重要的意义不是评测推荐系统，而是评测推荐系统的指标，设计是否合理，是否其高低反映了你预先的设定。

### 常用指标

#### 1. 系统有多好？

有两类，一类是深度类，一类是广度类

##### 深度类指标

就是看推荐系统在它的本职工作上做得如何。也就是预测用户和物品之间的连接，预测的方法又有评分预测和行为预测。

1. 评分准确度。通常就是均方根误差 RMSE，或者其他误差类指标，反映预测评分效果的好坏。在讲协同过滤时已经详细说过这个指标。这里不再赘述。
2.  排序。检测推荐系统排序能力非常重要，因为把用户偏爱的物品放在前面是推荐系统的天职。
3. 分类准确率。

通常检测推荐系统的商业指标有：点击率，转化率。

##### 广度类指标

4. 覆盖率。这项指标就是看推荐系统在多少用户身上开采成功了，覆盖率又细分为 UV 覆盖率和 PV 覆盖率

5. 失效率。失效率指标衡量推荐不出结果的情况。也分为 UV 失效率和 PV 失效率。

6. 新颖性。新颖性需要讲粒度，物品粒度、标签粒度、主题粒度、分类粒度等等。
7.  更新率。检测推荐结果更新程度。

#### 2. 系统健康指标

##### 1. 个性化。

取一天的日志，计算用户推荐列表的平均相似度。如果用户量较大，对用户抽样。

##### 2. 基尼系数。

基尼系数衡量推荐系统的马太效应，反向衡量推荐的个性化程度。

##### 3. 多样性。

多样性不但要在推荐服务吐出结果时需要做一定的保证，也要通过日志做监测。

### 攻击

攻击协同过滤，核心问题在于如何操纵选民。选民有两种，一种是用户，一种是物品，前者是基于用户的协同过滤所需要的，后者是基于物品的协同过滤所需要的。

方法就是操纵选民，这里的选民就是和被欺骗用户相似的用户。通常的手段就是批量制造假用户资料，并装作是与被欺骗用户兴趣相投的人。这叫做托攻击或者 Shilling Attacks，托也就是水军。

攻击手段包含这些元素。

1. 目标物品，就是攻击方要扶持或者打压的那个物品。
2. 助攻物品，就是用来构造假的相似用户所需要的物品。
3. 陪跑物品，就是用来掩饰造假的物品。

根据对最外环物品的评分构造方法不同，可以把攻击分为两种：

1. 随机攻击。随机打分就是用全局平均分构造一个正态分布，给无关物品打分时，用这个正态分布产生一个随机分值。
2. 平均分攻击。平均分攻击也是用在最外环物品中，给他们打每个物品的平均分。

更为狡猾的攻击办法，这里举两种，一种是热门攻击，还有一种是分段攻击。

热门攻击就是攻击者会想办法让目标物品和热门物品扯上关系。这样做有事半功倍的效果，热门物品有个特点是：评分用户多。使用假用户同时给热门物品和目标物品评上高分，这是针对扶持目标物品的做法，如果要打压，则给热门评高分，给目标物品最低分，陪跑物品就采用随机评分的方式。

热门攻击有两个“优势”：

1. 如果是扶持目标物品，则经过热门攻击后，基于物品的协同过滤算法会把目标物品计算为热门物品的相似物品
2. 基于用户的协同过滤算法，也会把消费过多个热门物品的用户计算为假用户的相似用户，从而为这些用户推荐出目标物品。

分段攻击就是想办法把目标物品引入到某个群体中，做法就是攻击者先圈定好用户群体，再列出这个群体肯定喜欢的物品集合，然后同时用假用户给目标物品和这批物品集合评分，做法类似热门攻击。

### 防护

按照层级，可以分为下面几种：

1. 平台级。一面是提高批量注册用户的成本，从攻击者的第一步遏制，比如弹验证码，另一方面是产品教育用户积极参与，并提供真实的反馈，让推荐系统所用的数据真实性比例越高，越不容易被攻击，这是最根本的

2. 数据级。数据级别防护重点是从数据中识别出哪些数据是假的，哪些用户是被操纵的选民，一旦识别出来就将这些数据删除。做法通常是采用机器学习思路，标注一批假用户或假反馈数据，训练分类器，在线上识别出反馈，将其延后或者排除在推荐计算之外，通常要和反垃圾系统紧密结合。或者对用户数据聚类，假用户产生的数据一定有着和正常用户不一样的分布，因为它目标明确，所以无监督的办法可以找出假用户群体来，一旦确认可以删除整个群体，可以采用的有主成分分析等做法。
3. 算法级。算法级别就是在推荐算法设计时，要根据情况做一些改进和选择。一般来说基于用户的协同过滤更容易受到攻击，因此需要对基于用户的协同过滤做改进。

改进方向包括下面几种：

1. 引入用户质量，限制对于低质量的用户参与计算，或者限制新用户参与计算；
2. 限制每个用户的投票权重，即在计算用户相似度时引入较重的平滑因子，使得用户之间的相似度不容易出现过高的值，也就是变相使得投票时参与用户更多一些，提高攻击者的成本。

### 开源工具及框架

#### 内容分析

文本结构化：主题模型；词嵌入；文本分类。

![](images\文本结构化工具.png)

#### 协同过滤和矩阵分解

KNN 相似度计算；SVD 矩阵分解；SVD++ 矩阵分解；ALS 矩阵分解；BPR 矩阵分解；低维稠密向量近邻搜索。

![](images\协同过滤和矩阵分解工具.png)

#### 模型融合

模型融合这部分，有线性模型、梯度提升树模型。

![](images\模型融合工具.png)

线性模型复杂在模型训练部分，这部分可以离线批量进行，而线上预测部分则比较简单，可以用开源的接口，也可以自己实现

#### 完整推荐系统

完整的推荐系统是指：包含推荐算法实现、存储、接口。



总结：

你可以选择各个模块的开源项目，再将其组合集成为自己的推荐系统。这样做的好处是有下面几种。

1. 单个模块开源项目容易入手，学习成本低，性能好；
2. 自己组合后更容易诊断问题，不需要的不用开发；
3. 单个模块的性能和效果更有保证。

## 参考资料

[推荐系统参考资料](推荐系统参考资料.md)