[Toc]

## 常见架构

### 信息流架构

要搜索 feed 相关的技术文章，你应该用“Activity Stream”作为关键词去搜，而不应该只用“feed”搜索

要实现一个信息流，可以划分为两个子问题。

1. 如何实现一个按照时间顺序排序的信息流系统？
2. 如何给信息流内容按照兴趣重排序？

整体架构：

![](images\信息流架构.png)

主要模块：日志收集、内容发布、机器学习、信息流服务、监控

#### 数据模型

信息流的基本数据有三个：用户（User）、内容（Activity）和关系（Connection）。

##### 1. 内容即 Activity。

用于表达 Activity 的元素有相应的规范，叫作 Atom。根据 Atom 规范的定义，一条 Activity 包含的元素有：Time、Actor、Verb、Object、Target、Title、Summary。

Actor：即“Activity 由谁发出的”，也就是和用户建立连接的另一端。

Verb：动词，就是连接的名字，比如“Follow”“Like”等，也可以是隐含的连接

Object：即动作作用到最主要的对象，只能有一个

Target：动作的最终目标，与 verb 有关，可以没有

举个例子： 2016 年 5 月 6 日 23:51:01（Time）你（Actor） 分享了（Verb） 一条动态（Object） 给 好友 （Target）。把前面这句话去掉括号后的内容就是它的 Title，Summary 暂略。

除了上面的字段外，还有一个隐藏的 ID，用于唯一标识一个 Activity

##### 2. 关系即连接。

定义一个连接的元素有下面几种。

- From：连接的发起方。
- To：被连接方。
- Type/Name：就是 Atom 模型中的 Verb，即连接的类型：关注、加好友、点赞、浏览、评论，等等。
- Affinity：连接的强弱。

#### 动态发布

**拉”模式**（Fan-out-on-load）

不足：

- 随着连接数的增加，这个操作的复杂度指数级增加；
- 内存中要保留每个人产生的内容；
- 服务很难做到高可用。

**“推”模式**（Fan-out-on-write）

不足：

- 大量的写操作：每一个粉丝都要写一次。
- 大量的冗余存储：每一条内容都要存储 N 份（受众数量）。
- 非实时：一条内容产生后，有一定的延迟才会到达受众信息流中。
- 无法解决新用户的信息流产生问题。

简单的结合方案：活跃度高的用户，使用推模式，活跃度没有那么高的用户，使用拉模式

Etsy 的设计方案：受众用户与内容产生用户之间的亲密度高，则优先推送，反之则推迟推送或者不推送。也不是完全遵循亲密度顺序，而是采用与之相关的概率。

对于信息流的产生和存储可以选择的工具有：

- 用户信息流的存储可以采用 Redis 等 KV 数据库, 使用 uid 作为 key。
- 信息流推送的任务队列可以采用 Celery 等成熟框架。

### 信息流排序

定义好互动行为，区分好正向互动和负向互动，典型的二分类监督学习。

能产生概率输出的二分类算法都可以用在这里，比如贝叶斯、最大熵、逻辑回归等。

建议小厂或者刚起步的信息流先采用线性模型。

对于线性模型，一个重要的工作就是特征工程。信息流的特征有三类：

- 用户特征，包括用户人口统计学属性、用户兴趣标签、活跃程度等；
- 内容特征，一条内容本身可以根据其属性提取文本、图像、音频等特征，并且可以利用主题模型提取更抽象的特征。
- 其他特征，比如刷新时间、所处页面等。

### 数据管道

信息流是一个数据驱动的系统，既要通过历史数据来寻找算法的最优参数，又要通过新的数据验证排序效果

管道中要使用的相关数据可能有：

1. 互动行为数据，用于记录每一个用户在信息流上的反馈行为；
2. 曝光内容，每一条曝光要有唯一的 ID，曝光的内容仅记录 ID 即可；
3. 互动行为与曝光的映射关系，每条互动数据要对应到一条曝光数据；
4. 用户画像内容，即用户画像，提供用户特征；
5. 信息流的内容分析数据，提供内容特征，即物品画像。



从零开始的信息流，没必要做到在线实时更新排序算法的参数。数据的管道可以分成三块：生成训练样本，可离线；排序模型训练，可离线；模型服务化，实时服务；



Facebook和头条的feed推荐区别：

共同点：
1. Facebook和今日头条都是要通过内容提取，用户和环境的分析找到最匹配的信息；
2. 根据用户的各种行为来衡量效果；
3. 都会引入一些无法完全用数据衡量的目标。比如屏蔽广告，屏蔽骚扰帐号，屏蔽有害内容；
4. 特征提取，特征匹配，用各种机器学习和深度学习的模型。用户标签/画像，内容标签的建立。这些工作后面的机制是相通的；
5. 实时信息流的更新量大，对性能要求高；
6. 都会有实验平台和长期跟踪效果的记录平台；
7. 都有人工介入评估。

不同点：

1. Facebook里原创和转发的动作比今日头条更频繁(我的理解)，这个动作的衡量会不同；
2. 今日头条的内容更复杂，种类更丰富，需要提取的特征种类，特征信息和衡量效果的因素更多；
3. 今日头条的内容是有层级逻辑关系的；
4. Facebook人之间的关系，互动的影响要比今日头条之间要大；
5. 今日头条内容团队的中国特色。评估效果时人工介入的更多。

### 架构的重要性

好的推荐系统架构应该具有这些特质：

1. 实时响应请求；

2. 及时、准确、全面记录用户反馈；

3. 可以优雅降级；

4. 快速实验多种策略。

   

Netflix 的推荐系统架构图：

![](images\Netflix的推荐系统架构图.png)

一共分成三层：

离线：不用实时数据，不提供实时服务；

近线：使用实时数据，不保证实时服务；

在线：使用实时数据，要保证实时服务。

#### 1. 数据流

行为事件数据，实时地被收集走，一边进入分布式的文件系统中存储，供离线阶段使用，另一边流向近线层的消息队列，供近线阶段的流计算使用。

#### 2. 在线层

哪些计算逻辑适合放在在线层呢？

简单的算法逻辑；模型的预测阶段；商业目标相关的过滤或者调权逻辑；场景有关的一些逻辑；互动性强的一些算法。

#### 3. 离线层

批量、周期性地执行一些计算任务。

![](images\离线层的示意图.png)



在 Netflix内部，Hermes管理数据源，要求：

1. 数据准备好之后及时通知相关方，也就是要有订阅发布的模式；
2. 能够满足下游不同的存储系统；
3. 完整的监控体系，并且监控过程对于数据使用方是透明的。

离线阶段的任务主要是两类：模型训练和推荐结果计算。

通常机器学习类模型，尤其是监督学习和非监督学习，都需要大量的数据和多次迭代，这类型的模型训练任务最适合放在离线阶段。

大多数推荐算法，实际上都是在离线阶段产生推荐结果的。离线阶段的推荐计算和模型训练，如果要用分布式框架，通常可以选择 Spark 等。

#### 4. 近线层

它结合了离线层和在线层的好处，摒弃了两者的不足。也叫做准实时层

近线计算任务一个核心的组件就是流计算，因为它要处理的实时数据流。常用的流计算框架有 Storm，Spark Streaming，FLink 等，Netflix 采用的内部流计算框架 Manhattan，这和 Storm 类似。

对比结果：

![](images\推荐架构三层对比.png)

### 简化架构

要从 0 开始搭建一个推荐系统，那么可以以 Netflix 的架构作为蓝本，做一定的简化。

![](images\简化架构.png)

关键简化有两点：

完全舍弃掉近线层；

避免使用分布式系统。

### 三种信息获取方式

搜索引擎，推荐系统，广告系统

![](images\信息获取方式.png)

### 架构抽象

我们抽象一下三者的需求共性：本质上都是在匹配，匹配用户的兴趣和需求（看成 context），但匹配的目标，条件和策略不尽相同。

抽象下去，又可以分为三步：过滤候选、排序候选、个性化输出。

#### 1. 过滤候选

#### 2.排序候选

候选排序这一步，对于三者来说，主要区别在于排序的目标和约束。

#### 3. 个性化输出

#### 4. 三者的协同