[Toc]

## 数据分析基础

### 数据分析总体概述

#### 数据采集

![](images\数据采集.jpg)

#### 数据挖掘

数据挖掘的核心是挖掘数据的商业价值，也就是我们所谈的商业智能 BI。

![](images\数据挖掘.jpg)

#### 数据可视化

![](images\数据可视化.jpg)

#### 修炼指南

我们只有把知识转化为自己的语言，它才真正变成了我们自己的东西

学习理论知识，提升认知；熟悉工具操作；多练习实践

### 数据挖掘学习路径

数据挖掘的基本流程、十大算法和数学原理

#### 基本流程

1. 商业理解：从商业的角度理解项目需求，再对数据挖掘的目标进行定义。
2. 数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等。这有助于你对收集的数据有个初步的认知。
3. 数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。
4. 模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
5. 模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。
6. 上线发布：呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程

#### 十大算法

分成四类:

- 分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CARTl 
- 聚类算法：K-Means，EMl 
- 关联分析：Aprioril 
- 连接分析：PageRank

##### 1. C4.5

C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理

##### 2.朴素贝叶斯（Naive Bayes）

对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。

##### 3. SVM

SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型

##### 4. KNN

KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。

##### 5. AdaBoost

Adaboost 在训练中建立了一个联合的分类模型，是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。

##### 6. CART

CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。

##### 7. Apriori

Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

##### 8. K-Means

K-Means 算法是一个聚类算法。

##### 9. EM

EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。

EM 算法经常用于聚类和机器学习领域中。

##### 10. PageRank

PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。PageRank 被 Google 创造性地应用到了网页权重的计算中，当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高

### 数据挖掘的数学原理

#### 1. 概率论与数理统计

比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。

#### 2. 线性代数

基于矩阵的各种运算，以及基于矩阵的理论，如PCA 方法、SVD 方法，以及 MF、NMF 方法

#### 3. 图论

#### 4. 最优化方法

最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。最优化方法的提出就是用更短的时间得到收敛，取得更好的效果。

总结：

![](images\数据挖掘知识清单.jpg)



### Python基础

#### NumPy

##### ndarray 对象

ndarray 实际上是多维数组的含义。在 NumPy 数组中，维数称为秩（rank），一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个轴（axes），秩就是描述轴的数量。

##### 结构数组

类似C中结构体的定义

```python
import numpy as np
#定义类型
persontype = np.dtype({ 'names':['name', 'age', 'chinese', 'math', 'english'], 'formats':['S32','i', 'i', 'i', 'f']})
#结构数组，每个元素都是一个persontype类型
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5), ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)], dtype=persontype)
#平均年龄
ages = peoples[:]['age']
print np.mean(ages)
```

##### ufunc 运算

能对数组中每个元素进行函数操作

**连续数组的创建**

```python
x1 = np.arange(1,11,2) #等差数组，初始值、终值、步长，默认是不包括终值
x2 = np.linspace(1,9,5) #初始值、终值、元素个数，默认是包括终值的
#x1,x2 都是[1 3 5 7 9]
```

**算数运算**

通过 NumPy 可以自由地创建等差数组，同时也可以进行加、减、乘、除、求 n 次方和取余数。

```python
x1 = np.arange(1,11,2)
x2 = np.linspace(1,9,5)
print np.add(x1, x2)
print np.subtract(x1, x2)
print np.multiply(x1, x2)
print np.divide(x1, x2)
print np.power(x1, x2)
#以下两个方法结果相同
print np.remainder(x1, x2)
print np.mod(x1, x2)
```

**统计函数**

```python
import numpy as np
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
#全部元素的最小值
print np.amin(a)
#amin(a,0) 是延着 axis=0 轴的最小值，也就是3个元素的最小值[1,2,3]
print np.amin(a,0)
#是延着axis=1 轴的最小值,[1,4,7]
print np.amin(a,1)
print np.amax(a)
print np.amax(a,0)
print np.amax(a,1)
```

统计最大值与最小值之差 **ptp()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.ptp(a)
print np.ptp(a,0)
print np.ptp(a,1)
```

统计数组的百分位数 **percentile()**

p 的取值范围是 0-100，如果 p=0，那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。

axis=0 是跨行（纵向），axis=1 是跨列（横向）

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.percentile(a, 50)
print np.percentile(a, 50, axis=0)
print np.percentile(a, 50, axis=1)
```

统计数组中的中位数 **median()**、平均数 **mean()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
#求中位数
print np.median(a)
print np.median(a, axis=0)
print np.median(a, axis=1)
#求平均数
print np.mean(a)
print np.mean(a, axis=0)
print np.mean(a, axis=1)
```

统计数组中的加权平均值 **average()**

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
#默认每个元素的权重是相同的
print np.average(a)
#加权平均
print np.average(a,weights=wts)
```

统计数组中的**标准差** std()、**方差 var()**

```
a = np.array([1,2,3,4])
print np.std(a)
print np.var(a)
```

#### NumPy 排序

sort(a, axis=-1, kind=‘quicksort’, order=None)

默认情况下使用的是快速排序；在 kind 里，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。同样 axis 默认是 -1，即沿着数组的最后一个轴进行排序，也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。另外 order 字段，对于结构化的数组可以指定按照某个字段进行排序。

```python
a = np.array([[4,3,2],[2,4,1]])
print np.sort(a)
print np.sort(a, axis=None)
print np.sort(a, axis=0)  
#[[2 3 1] [4 4 2]]
print np.sort(a, axis=1)  
#[[2 3 4] [1 2 4]]

persontype = np.dtype({ 'names':['name',  'chinese', 'math', 'english'], 'formats':['S32', 'i', 'i', 'f']})
#结构数组，每个元素都是一个persontype类型
peoples = np.array([("ZhangFei",75,100, 90),("GuanYu",85,96,88.5)], persontype)
#按总成绩排序
ranking = sorted(peoples,key=lambda x:x[1]+x[2]+x[3], reverse=True)
print(ranking)
```

### Pandas

Series 和 DataFrame 这两个核心数据结构，他们分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、统计和输出。

**Series 是个定长的字典序列。**说是定长是因为在存储的时候，相当于两个 ndarray.  Series 有两个基本属性：index 和 values。在 Series 结构中，index 默认是 0,1,2,……递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’, ‘d’]。

```python
import pandas as pd
#pip install pandas
from pandas import Series, DataFrame
x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
print x1
print x2
```

**DataFrame 类型数据结构类似数据库表。**它包括了行索引和列索引

```python
import pandas as pd
from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
df1= DataFrame(data)
df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])
print df1
print df2
```

**数据导入和输出** , 直接从 xlsx，csv 等文件中导入数据，也可以输出到 xlsx, csv 等文件

```python
import pandas as pd
from pandas import Series, DataFrame
#如果报缺少xlrd 和 openpyxl，使用pip install安装
score = DataFrame(pd.read_excel('data.xlsx'))
score.to_excel('data1.xlsx')
print score
```

#### 数据清洗

##### 1. 删除 DataFrame 中的不必要的列或行

```python
df2 = df2.drop(columns=['Chinese'])
df2 = df2.drop(index=['ZhangFei'])
```

##### 2. 重命名列名 columns，让列表名更容易识别

```python
df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)
```

##### 3. 去重复的值

```python
df = df.drop_duplicates() #去除重复行
```

##### 4. 格式问题

```python
#更改数据格式
df2['Chinese'].astype('str') 
df2['Chinese'].astype(np.int64) 

#删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.strip)
#删除左边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip)
#删除右边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip)
#删除特殊符号
df2['Chinese']=df2['Chinese'].str.strip('$')

#全部大写
df2.columns = df2.columns.str.upper()
#全部小写
df2.columns = df2.columns.str.lower()
#首字母大写
df2.columns = df2.columns.str.title()

#对数据表 df 进行 df.isnull()，可以打印所有位置是否空值
print(df.isnull())
#查看哪列存在空值，可以使用df.isnull().any()
print(df.isnull().any())
```

#### 使用 apply 函数对数据进行清洗

```python
#大写转化
df['name'] = df['name'].apply(str.upper)
#使用函数
def double_df(x):
           return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)

#新增两列，其中’new1’列是“语文”和“英语”成绩之和的 m 倍，'new2’列是“语文”和“英语”成绩之和的 n 倍
def plus(df,n,m):
    df['new1'] = (df[u'语文']+df[u'英语']) * m
    df['new2'] = (df[u'语文']+df[u'英语']) * n
    return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
```

#### 数据统计

![](images\统计函数.jpg)

describe() 函数最简便

```python
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print df1.describe()
```

#### 数据表合并

一个 DataFrame 相当于一个数据库的数据表，那么多个 DataFrame 数据表的合并就相当于多个数据库的表合并。

```python

df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
df2 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})
```

##### 1. 基于指定列进行连接

```python
df3 = pd.merge(df1, df2, on='name')
```

##### 2. inner 内连接

```python
df3 = pd.merge(df1, df2, how='inner')
```

##### 3. left 左连接

左连接是以第一个 DataFrame 为主进行的连接，第二个 DataFrame 作为补充。

```python
df3 = pd.merge(df1, df2, how='left')
```

##### 4. right 右连接

右连接是以第二个 DataFrame 为主进行的连接，第一个 DataFrame 作为补充。

```python
df3 = pd.merge(df1, df2, how='right')
```

##### 5. outer 外连接

外连接相当于求两个 DataFrame 的并集。

```python
df3 = pd.merge(df1, df2, how='outer')
```

#### pandasql:用SQL方式打开Pandas

pandasql 中的主要函数是 sqldf，它接收两个参数：一个 SQL 查询语句，还有一组环境变量 globals() 或 locals()。

```python
import pandas as pd
from pandas import DataFrame
from pandasql import sqldf, load_meat, load_births
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print pysqldf(sql)
```

![](images\Pandas.jpg)

#### 商业智能 BI、数据仓库 DW、数据挖掘 DM 三者之间的关系

##### 元数据 VS 数据元

**元数据**（MetaData）：描述其它数据的数据，也称为“中介数据”。

**数据元**（Data Element）：就是最小数据单元。

元数据可以很方便地应用于数据仓库.

#### 数据挖掘(KDD)的流程

##### 1. 分类

就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类

##### 2. 聚类

##### 3. 预测

##### 4. 关联分析

![](images\kdd过程.jpg)

数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换

1. 数据清洗

   主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。

2. 数据集成

   是将多个数据源中的数据存放在一个统一的数据存储中。

3.  数据变换

   将数据转换成适合数据挖掘的形式. 比如：归一化操作

4. 数据后处理。 是将模型预测的结果进一步处理后，再导出

### 用户画像

![](images\用户画像建模.jpg)

#### 用户唯一标识是整个用户画像的核心

设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。

#### 给用户打标签。

1. 用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。
2. 消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。
3. 行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。
4. 内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣

用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题

#### 业务价值

1. 获客：如何进行拉新，通过更精准的营销获取客户。
2. 粘客：个性化推荐，搜索排序，场景运营等。
3. 留客：流失率预测，分析关键节点降低流失率。

![](images\标签化的流程.jpg)

#### 美团外卖分析

打标签：

1. 用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。
2. 消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。
3. 行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。
4. 内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

业务价值：

1. 在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。
2. 在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。
3. 在留客上，预测用户是否可能会从平台上流失。

### 数据采集

![](images\数据源.jpg)

开放数据源：单位维度，行业维度

![](images\单位维度的数据源.jpg)

#### 爬虫爬取

经历三个过程：使用 Requests 爬取内容；使用 XPath 解析内容；使用 Pandas 保存数据；

**抓取工具**

**火车采集器**：

数据抓取，清洗、数据分析、数据挖掘和可视化等工作。数据源适用于绝大部分的网页。

**八爪鱼**：

免费的采集模板实际上就是内容采集规则，包括了电商类、生活服务类、社交媒体类和论坛类的网站都可以采集，用起来非常方便。当然你也可以自己来自定义任务。

> 输入网页，设计流程，启动采集
>
> 自定义采集：打开网页、点击元素、循环翻页、提取数据
>
> 两个重要的工具一定要用好：流程视图和 XPath。

云采集收费，配置好采集任务，通过云端多节点并发采集，采集速度远远超过本地采集。可以自动切换多个 IP，避免 IP 被封。

**集搜客**：

完全可视化操作，无需编程。没有云采集功能

#### 日志采集

Web 服务器采集，自定义采集用户行为（js，AJAX）

#### 埋点

第三方的工具，比如友盟、Google Analysis、Talkingdata 等

#### 爬虫的流程

##### Requests 访问页面

```python
r = requests.get('http://www.douban.com')
r = requests.post('http://xxx.com', data = {'key':'value'})
```

##### XPath 定位

![](images\XPath路径表达式.jpg)

```python
#例子：
xpath(‘node’) #选取了 node 节点的所有子节点；
xpath(’/div’) #从根节点上选取 div 节点；
xpath(’//div’) #选取所有的 div 节点；
xpath(’./div’) #选取当前节点下的 div 节点；
xpath(’…’) #回到上一个节点；
xpath(’//@id’) #选取所有的 id 属性；
xpath(’//book[@id]’)#选取所有拥有名为 id 的属性的book元素；
xpath(’//book[@id=“abc”]’)#选取所有book元素，且这些book元素拥有id="abc"的属性；
xpath(’//book/title|//book/price’)#选取book元素的所有title和price元素。

from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')#所有列表项目
```

JSON对象转化

```python
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)#Json对象转Python对象
json.dumps(input)#Python对象转JSON对象
print input
```

**如何使用 JSON 数据自动下载海报**：

```python
# coding:utf-8
import requests
import json
query = '王祖贤'
headers = {'User-Agent': 'Mozilla/5.0 ...'}
''' 下载图片 '''
def download(src, id):
  #去除字符串空格，最好也去掉特殊符号，如果目录不确定存在，最好用os方法判断创建
  dir = './' + str(id).replace(' ','') + '.jpg'
  try:
    pic = requests.get(src, timeout=10)
    fp = open(dir, 'wb')
    fp.write(pic.content)
    fp.close()
  except requests.exceptions.ConnectionError:
    print('图片无法下载')
            
''' for 循环 请求全部的 url '''
for i in range(0, 22471, 20):
  url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
  html = requests.get(url,  header).text    # 得到返回结果
  response = json.loads(html,encoding='utf-8') # 将 JSON 格式转换成 Python 对象
  for image in response['images']:
    print(image['src']) # 查看当前下载的图片网址
    download(image['src'], image['id']) # 下载一张图片
```

**使用 XPath 自动下载电影海报**

一个快速定位 XPath 的方法就是采用浏览器的 XPath Helper 插件，使用 Ctrl+Shift+X 快捷键的时候，用鼠标选中你想要定位的元素，就会得到类似下面的结果

有时候你需要Selenium 库工具，来进行网页加载的模拟，直到完成加载后它才给你完整的 HTML。

```python
from lxml import etree
from selenium import webdriver

request_url ='https://search.douban.com/movie/subject_search?search_text=%E7%8E%8B%E7%A5%96%E8%B4%A4&cat=1002'
#这个参数规则看网页结构调整
src_xpath = "//div[@class='item-root']/a[@class='cover-link']/img[@class='cover']/@src"
title_xpath = "//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']"

driver = r"E:\pythonworkspace\drivers\chromedriver.exe"

driver = webdriver.Chrome(driver)
driver.get(request_url)
html = etree.HTML(driver.page_source)
driver = webdriver.Chrome()
driver.get(request_url)

#获取到完整的HTML时，就可以对HTML中的 XPath 进行提取
''' for 循环 请求全部的 url '''
for i in range(0, 45, 15):
    url = request_url + '&start=' + str(i)
    driver.get(url)
    html = etree.HTML(driver.page_source)
    srcs = html.xpath(src_xpath)
    titles = html.xpath(title_xpath)
    print(srcs, titles)
    for image, title in zip(srcs, titles):
        download(image, title.text)  # 下载一张图片
```

ChromeDriver镜像站：https://chromedriver.storage.googleapis.com/index.html

### 数据清洗

#### 数据清洗规则：

1. 完整性：单条数据是否存在空值，统计的字段是否完善。
2. 全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
3. 合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。
4. 唯一性：数据是否存在重复记录，行数据、列数据都需要是唯一的。

#### 使用 Pandas 来进行清洗

##### 1. 完整性

**缺失值处理**：

- 删除：删除数据缺失的记录；
- 均值：使用当前列的均值；
- 高频：使用当前列出现频率最高的数据

```python
df['Age'].fillna(df['Age'].mean(), inplace=True)

age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True)
```

**空行处理**：

```python
# 删除全空的行
df.dropna(how='all',inplace=True) 
```

##### 2. 全面性

**单位统一**：

```python
# 获取 weight 数据列中单位为lbs的数据，磅（lbs）转化为千克（kgs）
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
print df[rows_with_lbs]
# 将 lbs转换为 kgs, 2.2lbs=1kgs
for i,lbs_row in df[rows_with_lbs].iterrows():
  # 截取从头开始到倒数第三个字符之前，即去掉lbs。
  weight = int(float(lbs_row['weight'][:-3])/2.2)
  df.at[i,'weight'] = '{}kgs'.format(weight) 
```

##### 3. 合理性

**非 ASCII 字符处理**

```python
# 删除非 ASCII 字符
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

##### 4. 唯一性

一列有多个参数，做拆分：

```python
# 切分名字，删除源数据列
df[['first_name','last_name']] = df['name'].str.split(expand=True)
df.drop('name', axis=1, inplace=True)
```

重复数据：

```python
# 删除重复数据行
df.drop_duplicates(['first_name','last_name'],inplace=True)
```

**没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障**。



**数据清洗小作业**：

分析：NaN，负值删除记录，大小写统一，同名数据合并，

| food        | ounces | animal |
| ----------- | ------ | ------ |
| bacon       | 4.0    | pig    |
| pulled pork | 3.0    | pig    |
| bacon       | NaN    | pig    |
| Pastrami    | 6.0    | cow    |
| corned beef | 7.5    | cow    |
| Bacon       | 8.0    | pig    |
| pastrami    | -3.0   | cow    |
| honey ham   | 5.0    | pig    |
| nova lox    | 6.0    | salmon |

### 数据集成

#### 两种架构：ELT 和 ETL

ETL 的过程为提取 (Extract)——转换 (Transform)——加载 (Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地。

ELT 的过程则是提取 (Extract)——加载 (Load)——变换 (Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

**目前**数据集成的**主流架构是 ETL**，但**未来**使用 **ELT** 作为数据集成架构的将越来越多。这样做会带来多种好处：

1. ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
2. 在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程

#### ETL 工具

典型的 ETL 工具有:

1. 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等
2. 开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等

国内很多公司都在使用 Kettle 用来做数据集成

##### Kettle 工具的使用

Kettle，纯 Java 编写，在 2006 年并入了开源的商业智能公司 Pentaho, 正式命名为 Pentaho Data Integeration，简称“PDI”。因此 Kettle 现在是 Pentaho 的一个组件，下载地址：https://community.hitachivantara.com/docs/DOC-1009855

Kettle 采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。

Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。

Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。

**创建 Transformation**：

Transformation 可以分成三个步骤，它包括了输入、中间转换以及输出。

在 Transformation 中包括两个主要概念：Step 和 Hop。Step 的意思就是步骤，Hop 就是跳跃线的意思。

- Step（步骤）：Step 是转换的最小单元，每一个 Step 完成一个特定的功能。在上面这个转换中，就包括了表输入、值映射、去除重复记录、表输出这 4 个步骤；
- Hop（跳跃线）：用来在转换中连接 Step。它代表了数据的流向。

**创建 Job（作业）**：

完整的任务，实际上是将创建好的转换和作业串联起来。 Job 包括两个概念：Job Entry、Hop。

- Job Entry（工作实体）：Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。
- Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。

在 Kettle 中，你可以使用 **Spoon**，它是一种一种图形化的方式，来让你设计 Job 和 Transformation，并且可以保存为文件或者保存在数据库中。

###### 案例 1：将文本文件的内容转化到 MySQL 数据库

| create_time | name       | Chinese | English | Math |
| ----------- | ---------- | ------- | ------- | ---- |
| 2018/12/22  | ZhangFei   | 66      | 65      | 30   |
| 2018/12/22  | GuanYu     | 95      | 85      | 98   |
| 2018/12/22  | ZhaoYun    | 93      | 92      | 96   |
| 2018/12/22  | HuangZhong | 90      | 88      | 77   |
| 2018/12/22  | DianWei    | 80      | 90      | 90   |

Step1：创建转换，右键“转换→新建”；

Step2：在左侧“核心对象”栏目中选择“文本文件输入”控件，拖拽到右侧的工作区中；

Step3：从左侧选择“表输出”控件，拖拽到右侧工作区；

Step4：鼠标在“文本文件输入”控件上停留，在弹窗中选择图标，鼠标拖拽到“表输出”控件，将一条连线连接到两个控件上；这时我们已经将转换的流程设计好了，现在是要对输入和输出两个控件进行设置。

Step5：双击“文本文件输入”控件，导入已经准备好的文本文件；

Step6：双击“表输出”控件，这里你需要配置下 MySQL 数据库的连接，同时数据库中需要有一个数据表，字段的设置与文本文件的字段设置一致（这里我设置了一个 wucai 数据库，以及 score 数据表。字段包括了 name、create_time、Chinese、English、Math，与文本文件的字段一致）。

Step7：创建数据库字段的对应关系，这个需要双击“表输出”，找到数据库字段，进行字段映射的编辑；

Step8：点击左上角的执行图标

Kettle 的开源社区：http://www.ukettle.org

##### 阿里开源软件：DataX

DataX 可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。

DataX 的模式是基于框架 + 插件完成的，DataX 的框架如下图：

![](images\DataX的框架.jpg)

将数据从源头装载到 DataXStorage，然后在“写”模块，数据从 DataXStorage 导入到目的地。

这样的好处就是，在整体的框架下，我们可以对 Reader 和 Writer 进行插件扩充，比如我想从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和 OracleWriter 插件，装在框架上使用即可。

##### Apache 开源软件:Sqoop

由 Apache 基金会所开发的分布式系统基础架构，它主要用来在 Hadoop 和关系型数据库中传递数据

### 数据变换

数据变换是数据准备的重要环节，它通过**数据平滑、数据聚集、数据概化和规范化**等方式将数据转换成适用于数据挖掘的形式

常见的变换方法：

1. 数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑；
2. 数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；
3. 数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。
4. 数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等；
5. 属性构造：构造出新的属性并添加到属性集中。

#### 数据规范化的几种方法

##### 1. Min-max 规范化

Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：

新数值 =（原数值 - 极小值）/（极大值 - 极小值）。

##### 2. Z-Score 规范化

优点：算法简单，不受数据量级影响，结果易于比较。不足：它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较

新数值 =（原数值 - 均值）/ 标准差

##### 3. 小数定标规范化

小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。

#### Python 的 SciKit-Learn 库使用

SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块

1. Min-max 规范化

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征，按列计算
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化，MinMaxScaler原始数据投射到指定的空间[min, max]，默认0-1
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print minmax_x
```

2. Z-Score 规范化

```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行Z-Score规范化
scaled_x = preprocessing.scale(x)
print scaled_x

#结果：
[[-0.70710678 -1.41421356  0.26726124]
 [ 1.41421356  0.70710678  1.06904497]
 [-0.70710678  0.70710678 -1.33630621]]
```

3. 小数定标规范化

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 小数定标规范化
j = np.ceil(np.log10(np.max(abs(x))))
scaled_x = x/(10**j)
print scaled_x
```

**注意**：这些算法都需要是多行，如果一行多列会是0，如果一维会报错

### 数据可视化

可视化视图超过 20 种，分别包括：文本表、热力图、地图、符号地图、饼图、水平条、堆叠条、并排条、树状图、圆视图、并排圆、线、双线、面积图、双组合、散点图、直方图、盒须图、甘特图、靶心图、气泡图等。

9 种情况：

![](images\可视化的视图使用.png)

#### 商业智能分析

最著名的当属 Tableau 和 PowerBI 了，另外中国帆软出品的 FineBI 也受到国内很多企业的青睐。

Tableau 是国外的商业软件，收费不低。它适合 BI 工程师、数据分析分析师。

PowerBI 是微软出品的，可以和 Excel 搭配使用，你可以通过 PowerBI 来呈现 Excel 的可视化内容。

#### 可视化大屏类

##### DataV

DataV 是一款可视化的工具，天猫双十一大屏就是用它呈现的。你要做的就是选择相应的控件，配置控件的样式、数据传输和交互效果等。当然 DataV 本身有一些免费的模板，你可以直接通过模板来创建

##### FineReport

FineReport 是帆软出品的工具，可以做数据大屏，也可以做可视化报表，在很多行业都有解决方案，操作起来也很方便。可以实时连接业务数据，对数据进行展示。

#### 前端可视化组件

Canvas 和 SVG 是 HTML5 中主要的 2D 图形技术，WebGL 是 3D 框架。

**Canvas** 适用于位图，也就是给了你一张白板，需要你自己来画点。Canvas 技术可以绘制比较复杂的动画。不过它是 HTML5 自带的，所以低版本浏览器不支持 Canvas。ECharts 这个可视化组件就是基于 Canvas 实现的。

**SVG** 的中文是可缩放矢量图形，它是使用 XML 格式来定义图形的。相当于用点和线来描绘了图形，相比于位图来说文件比较小，而且任意缩放都不会失真。SVG 经常用于图标和图表上。它最大的特点就是支持大部分浏览器，动态交互性实现起来也很方便，比如在 SVG 中插入动画元素等。

**WebGL** 是一种 3D 绘图协议，能在网页浏览器中呈现 3D 画面技术，并且可以和用户进行交互。你在网页上看到的很多酷炫的 3D 效果，基本上都是用 WebGL 来渲染的。下面介绍的 Three.js 就是基于 WebGL 框架的

**可视化组件**： **Echarts、D3、Three.js 和 AntV**

ECharts 是基于 H5 canvas 的 Javascript 图表库，是百度的开源项目，一直都有更新，使用的人也比较多。它作为一个组件，可以和 DataV、Python 进行组合使用。你可以在 DataV 企业版中接入 ECharts 图表组件。也可以使用 Python 的 Web 框架（比如 Django、Flask）+ECharts 的解决方案

D3 的全称是 Data-Driven Documents，是一个 JavaScript 的函数库。它提供了各种简单易用的函数，大大简化了 JavaScript 操作数据的难度。你只需要输入几个简单的数据，就能够转换为各种绚丽的图形

Three.js，顾名思义，就是 Three+JS 的意思。“Three.js”使用 JavaScript 来实现 3D 效果。Three.js 是一款 WebGL 框架，封装了大量 WebGL 接口。

AntV 是蚂蚁金服出品的一套数据可视化组件，包括了 G2、G6、F2 和 L7 一共 4 个组件。其中 G2 应该是最知名的，它的意思是 The grammar Of Graphics，也就是一套图形语法。它集成了大量的统计工具，而且可以让用户通过简单的语法搭建出多种图表。G6 是一套流程图和关系分析的图表库。F2 适用于移动端的可视化方案。L7 提供了地理空间的数据可视化框架。

#### 编程语言

在 Python 里包括了众多可视化库，比如 Matplotlib、Seaborn、Bokeh、Plotly、Pyecharts、Mapbox 和 Geoplotlib。其中使用频率最高，最需要掌握的就是 Matplotlib 和 Seaborn。

**Matplotlib** 是 Python 的可视化基础库，作图风格和 MATLAB 类似，所以称为 Matplotlib。一般学习 Python 数据可视化，都会从 Matplotlib 入手，然后再学习其他的 Python 可视化库。

**Seaborn** 是一个基于 Matplotlib 的高级可视化效果库，针对 Matplotlib 做了更高级的封装，让作图变得更加容易。你可以用短小的代码绘制更多维度数据的可视化效果图

在 R语言中也有很多可视化库可供选择。其中包括了 R 自带的绘图包 Graphics 以及工具包 ggplot2、ggmap、timevis 和 plotly 等。其中 **ggplot2** 是 R 语言中重要的绘图包，这个工具包将数据与绘图操作进行了分离，所以使用起来清晰明了，画出的图也漂亮。其实在 **Python** 里后来也引入了 **ggplot** 库

#### 数据可视化的学习

1. 重点推荐 TableauTableau 

   在可视化灵活分析上功能强大，主要目标用户更多是较专业的数据分析师。同时在工作场景中使用率高，因此掌握 Tableau 对于晋升和求职都很有帮助。不过 Tableau 是个商业软件，收费不低。而且上手起来有一些门槛，需要一定数据基础

2. 使用微图、DataV

   微图和八爪鱼是一家公司的产品，使用起来非常方便，而且免费。当你用八爪鱼采集数据之后，就直接可以用微图进行数据可视化。

   DataV 是阿里推出的数字大屏技术，不过它是收费的产品。它最大的好处，就是可以分享链接，让别人可以在线浏览，不需要像 Tableau 一样安装客户端才能看到数据可视化的结果。另外 DataV 有一些模板，你直接可以使用。

3. Python 可视化

1. 